{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Wikipedia \n",
    "\n",
    "This notebook's aim is to extract different tags located on a Wikipedia page. In this example we will be mostly focusing on the information within the \"paragraph\" tag in Wikipedia. Although this example is mainly focused on using Wikipedia as an example. Aspects of it can be dissected out to work on other website. This will cover how to read a URL, use Beautiful Soup 4 and Python 3 to extract/parse different tags inside Wikipedia.\n",
    "\n",
    "## Libraries and Resources used\n",
    "\n",
    "-  Python 3\n",
    "-  Beautiful Soup 4 and some of its related libraries \n",
    "-  Request (A simple HTTP library for Python)\n",
    "\n",
    "## Note:\n",
    "\n",
    "For installation of the nessesary resources and libraries refer to their respective home page for installation steps for your operation system.\n",
    "\n",
    "In this tutorial we will be using a hyperlinks from Wikipedia. It is possible when you are viewing this example, there have been changes done to the URL in question. If that is true, this may result in your own implementation of this code to produce different results or throwing an error.\n",
    "\n",
    "Written in September 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the requried libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# request handles the URL management\n",
    "import requests\n",
    "\n",
    "# Import the different funtionality from Beautiful Soup and related modules\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the desired URL\n",
    "\n",
    "In this example we are going to use a wikipedia page about Web Scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gets the HTML information about the web page\n",
    "webPage = requests.get('https://en.wikipedia.org/wiki/Web_scraping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Beauitful Soup Object\n",
    "\n",
    "A Beauitful Soup object is a special type of object that allows us to perform a varitity of different operations. This libaray allows us to extract key information/aspects about the page such as tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we create a soup object using the text from the webPage we declared eariler\n",
    "# 'html.parser' is used to tell the library that we are parsing a HTML\n",
    "soup = BeautifulSoup(webPage.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating through Tags\n",
    "\n",
    "As you can notice on any Wikipedia page there are hyperlinks inside the content of a page. From these hyperlinks it will bring you to another Wikipedia page. However if we look at the HTML of a \"paragraph\" tag inside a Wikipedia page below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><b>Web scraping</b>, <b>web harvesting</b>, or <b>web data extraction</b> is <a href=\"/wiki/Data_scraping\" title=\"Data scraping\">data scraping</a> used for <a href=\"/wiki/Data_extraction\" title=\"Data extraction\">extracting data</a> from <a href=\"/wiki/Website\" title=\"Website\">websites</a>.<sup class=\"reference\" id=\"cite_ref-Boeing2016JPER_1-0\"><a href=\"#cite_note-Boeing2016JPER-1\">[1]</a></sup> Web scraping software may access the World Wide Web directly using the <a href=\"/wiki/Hypertext_Transfer_Protocol\" title=\"Hypertext Transfer Protocol\">Hypertext Transfer Protocol</a>, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a <a href=\"/wiki/Internet_bot\" title=\"Internet bot\">bot</a> or <a href=\"/wiki/Web_crawler\" title=\"Web crawler\">web crawler</a>. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local <a href=\"/wiki/Database\" title=\"Database\">database</a> or spreadsheet, for later <a href=\"/wiki/Data_retrieval\" title=\"Data retrieval\">retrieval</a> or <a href=\"/wiki/Data_analysis\" title=\"Data analysis\">analysis</a>.</p>\n"
     ]
    }
   ],
   "source": [
    "# Prints the 1st paragraph tag inside the soup object\n",
    "print(soup.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see inside the \"paragraph\" tag there exist more embedded tags such as the \"anchor\" tag, etc. Therefore we would like to clean it up such that we are left with just the text and none of the meta data and HTML tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning all Tags inside the HTML Page\n",
    "\n",
    "If the goal is to remove all the Tags and remove the metadata assoicated with the HTML page. This would be the simpliest method to do so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis. \n",
      "\n",
      "Web scraping a web page involves fetching it and extracting from it.[1][2] Fetching is the downloading of a page (which a browser does when you view the page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and phone numbers, or companies and their URLs, to a list (contact scraping). \n",
      "\n",
      "Web scraping is used for contact scraping, and as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup and, web data integration. \n",
      "\n",
      "Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form. However, most web pages are designed for human end-users and not for ease of automated use. Because of this, tool kits that scrape web content were created. A web scraper is an Application Programming Interface (API) to extract data from a web site. Companies like Amazon AWS and Google provide web scraping tools, services and public data available free of cost to end users. \n",
      "\n",
      "Newer forms of web scraping involve listening to data feeds from web servers. For example, JSON is commonly used as a transport storage mechanism between the client and the web server. \n",
      "\n",
      "There are methods that some websites use to prevent web scraping, such as detecting and disallowing bots from crawling (viewing) their pages. In response, there are web scraping systems that rely on using techniques in DOM parsing, computer vision and natural language processing to simulate human browsing to enable gathering web page content for offline parsing. \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Web scraping is the process of automatically mining data or collecting information from the World Wide Web. It is a field with active developments sharing a common goal with the semantic web vision, an ambitious initiative that still requires breakthroughs in text processing, semantic understanding, artificial intelligence and human-computer interactions. Current web scraping solutions range from the ad-hoc, requiring human effort, to fully automated systems that are able to convert entire web sites into structured information, with limitations. \n",
      "\n",
      "Sometimes even the best web-scraping technology cannot replace a humanâ€™s manual examination and copy-and-paste, and sometimes this may be the only workable solution when the websites for scraping explicitly set up barriers to prevent machine automation. \n",
      "\n",
      "A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression-matching facilities of programming languages (for instance Perl or Python). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find all the tags you are want to save\n",
    "# In this example I am only concern with the \"paragraph\", \"list item\" and \"header 1\" tag\n",
    "# If you want everything you can just use \"soup.find_all()\" with nothing declared in the parameters\n",
    "wikiContent = soup.find_all([\"p\"])\n",
    "\n",
    "# Now to check to see the result\n",
    "# Print out all the item to check the result\n",
    "count = 11\n",
    "\n",
    "for item in wikiContent:\n",
    "    # Display on the first 10 results\n",
    "    if count > 0:\n",
    "        # Get the item's text (does not return the metadata)\n",
    "        print(item.get_text() + \" \\n\")\n",
    "        count = count - 1\n",
    "    else:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have removed all the metadata and tags assoicated with the HTML and we are left with just the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving into a Text File\n",
    "\n",
    "Now that we have aquired a cleaned version of the HTML we can now save it into a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open a text file to save it into (if it doesn't exist it will create one)\n",
    "outputfile = open( 'cleanedHTML.txt', 'w' )\n",
    "\n",
    "# For every item we declared eariler in wikiContent\n",
    "for item in wikiContent:\n",
    "    \n",
    "    # Write into the file the text and end it with a \"newline\" \n",
    "    # If you do not have the division between the elements remove the \"+ \" \\n\" inside the bracket\n",
    "    outputfile.write(item.get_text() + \" \\n\")\n",
    "\n",
    "# Close the file\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Now you know how to clean a Wikipedia Page and write the cleaned HTML into a text file for future use. In the sections below will show other methods to clean the HTML. Depending on your needs this may or may not be relavent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing What You Need\n",
    "\n",
    "If you are only concern with all the \"paragraph\" tags in a webpage, there is a method in which we can just extract the \"paragraph\" tags. This might be relavent in the next section, since we do not want to clean what we are not going to use. Therefore in the next few sections we are going to need to import some additional modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The additional modules we are going to need\n",
    "from bs4 import SoupStrainer, NavigableString, Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating A More Filtered Soup Object\n",
    "\n",
    "In this section we are going to show how to create a soup object that only contains one type of tag. To do so we are going to use SoupStrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the basic soup object previously seen\n",
    "oldSoup = BeautifulSoup(webPage.text, 'html.parser')\n",
    "\n",
    "# Create a parser that looks for one particular tag in the HTML\n",
    "# In this example it is the \"paragraph\" tag\n",
    "only_p_tags = SoupStrainer(\"p\")\n",
    "\n",
    "# This creates the soup object under the condition of the parser\n",
    "newSoup = BeautifulSoup(webPage.text, 'html.parser', parse_only = only_p_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Beautiful Object \n",
    "\n",
    "This involves the manual removal of the tags presented inside the soup object. The following example words with the old soup object as well as the new one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Original\n",
    "\n",
    "First lets see what the first \"paragraph\" tag in the original soup object contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><b>Web scraping</b>, <b>web harvesting</b>, or <b>web data extraction</b> is <a href=\"/wiki/Data_scraping\" title=\"Data scraping\">data scraping</a> used for <a href=\"/wiki/Data_extraction\" title=\"Data extraction\">extracting data</a> from <a href=\"/wiki/Website\" title=\"Website\">websites</a>.<sup class=\"reference\" id=\"cite_ref-Boeing2016JPER_1-0\"><a href=\"#cite_note-Boeing2016JPER-1\">[1]</a></sup> Web scraping software may access the World Wide Web directly using the <a href=\"/wiki/Hypertext_Transfer_Protocol\" title=\"Hypertext Transfer Protocol\">Hypertext Transfer Protocol</a>, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a <a href=\"/wiki/Internet_bot\" title=\"Internet bot\">bot</a> or <a href=\"/wiki/Web_crawler\" title=\"Web crawler\">web crawler</a>. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local <a href=\"/wiki/Database\" title=\"Database\">database</a> or spreadsheet, for later <a href=\"/wiki/Data_retrieval\" title=\"Data retrieval\">retrieval</a> or <a href=\"/wiki/Data_analysis\" title=\"Data analysis\">analysis</a>.</p>\n"
     ]
    }
   ],
   "source": [
    "print(oldSoup.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see if we can't replace all the \"anchor\" tags with what we see on the original page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code aims to remove all the anchor tags\n",
    "# Iterate through all the anchor tags\n",
    "while True:\n",
    "    \n",
    "    # If there is no more answer we are done\n",
    "    if oldSoup.a == None:\n",
    "        break\n",
    "        \n",
    "    # If there is no string for the URL just insert a white space\n",
    "    elif oldSoup.a.string == None:\n",
    "        oldSoup.a.replace_with(\" \")\n",
    "    \n",
    "    # Replace the anchor tag with the string inside\n",
    "    else:\n",
    "        # Adding spaces before and after the words\n",
    "        newFormat = \" \" + str(oldSoup.a.string) + \" \"\n",
    "        oldSoup.a.replace_with(newFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above removes all the metadata and information inside the \"anchor\" tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><b>Web scraping</b>, <b>web harvesting</b>, or <b>web data extraction</b> is  data scraping  used for  extracting data  from  websites .<sup class=\"reference\" id=\"cite_ref-Boeing2016JPER_1-0\"> [1] </sup> Web scraping software may access the World Wide Web directly using the  Hypertext Transfer Protocol , or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a  bot  or  web crawler . It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local  database  or spreadsheet, for later  retrieval  or  analysis .</p>\n"
     ]
    }
   ],
   "source": [
    "print(oldSoup.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see. Compared to the previous example we have now removed all the \"anchor\" tags that existed. This exact process can be repeated for all other types as well. For Example what if we also wanted to remove all the \"b\" tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    # The previous code\n",
    "    if oldSoup.a == None:\n",
    "        break\n",
    "    elif oldSoup.a.string == None:\n",
    "        oldSoup.a.replace_with(\" \")\n",
    "    else:\n",
    "        newFormat = \" \" + str(oldSoup.a.string) + \" \"\n",
    "        oldSoup.a.replace_with(newFormat)\n",
    "    \n",
    "    # If there is no more answer we are done\n",
    "    if oldSoup.b == None:\n",
    "        break\n",
    "        \n",
    "    # If there is no string for the URL just insert a white space\n",
    "    elif oldSoup.b.string == None:\n",
    "        oldSoup.b.replace_with(\" \")\n",
    "    \n",
    "    #Replace the anchor tag with the string inside\n",
    "    else:\n",
    "        #Adding spaces before and after the words\n",
    "        newFormat = \" \" + str(oldSoup.b.string) + \" \"\n",
    "        oldSoup.b.replace_with(newFormat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><b>Web scraping</b>, <b>web harvesting</b>, or <b>web data extraction</b> is  data scraping  used for  extracting data  from  websites .<sup class=\"reference\" id=\"cite_ref-Boeing2016JPER_1-0\"> [1] </sup> Web scraping software may access the World Wide Web directly using the  Hypertext Transfer Protocol , or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a  bot  or  web crawler . It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local  database  or spreadsheet, for later  retrieval  or  analysis .</p>\n"
     ]
    }
   ],
   "source": [
    "print(oldSoup.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore this could be expanded to encompass whatever tags you wish to remove. All that is need is to add additional if statements for the desired tags in the while loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving into Text File\n",
    "\n",
    "Unlike how it was done eariler, there is some slight alteration that need to be done in order to save this new version. The reason for this is in the previous example we used a prebuilt method that stripped all the tags, however in this case we only removed two (the \"a\" and \"b\" tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following the previous example of gathering only the \"p\", \"li\" and \"h1\" tag\n",
    "parsedWikiContent = oldSoup.find_all([\"p\",\"li\",\"h1\"])\n",
    "\n",
    "# Open a text file to save it into (if it doesn't exist it will create one)\n",
    "outputfile = open( 'selectiveCleanHTML.txt', 'w' )\n",
    "\n",
    "# For every item we declared eariler in wikiContent\n",
    "for item in parsedWikiContent:\n",
    "    \n",
    "    # Convert the objects to string so we can write it to file\n",
    "    outputfile.write(str(item) + \" \\n\")\n",
    "\n",
    "# Close the file\n",
    "outputfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
