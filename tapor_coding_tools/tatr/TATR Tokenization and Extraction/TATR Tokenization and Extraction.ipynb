{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATR: Tokenization and Extraction\n",
    "\n",
    "This notebook is part of a greater series of Juypter Notebook structured around Twitter Tweet analysis. This particular notebook will look at tokenization of extracting key features from a tweet text. This notebook also serves as one of the introductory notebook for TATR as tokenization and extraction are fundamental features for any analysis. \n",
    "\n",
    "Any additional assumptions and clarification will be discussed and declared throughout the notebook.\n",
    "\n",
    "### Note: \n",
    "This notebook recommend you look at TATR: Panda and CSV of Tweets. This is because we will be using some of the features discussed in that notebook. \n",
    "\n",
    "Written 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Tokenization\n",
    "\n",
    "Tokenization is the process in which we seperate a string into parts. Whether that be by words, sentences or some other rubric. Therefore the tokenization is important when segmenting your twitter data. This will be further elaborated when tokenization comes up. \n",
    "\n",
    "To find out more about tokenization in general:\n",
    "* https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Now we will import all the Python 3 libraries that will be used in this notebook. You do not need to know all the functionalities of each libraries as some are massive. However any functionalities that is used will be explained as they appear, therefore do not worry too much if you do not recongize the libraries. \n",
    "\n",
    "To import or download the required libraries see the Juypter documentation or the libraries's home page for instruction. \n",
    "\n",
    "### Note: \n",
    "All libraries that are used are available for Anaconda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing data structure libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import text analysist tools\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using NLTK\n",
    "\n",
    "In this notebook we will look at using NLTK (Natural Language Toolkit) for tokenization. NLTK is a popular and powerful Python library when working with human language data. The library contains many different functionalities from tokenization to word classification. In this notebook we will just be examining the tokenization features of NLTK\n",
    "\n",
    "To find out more about NLTK see:\n",
    "* http://www.nltk.org/\n",
    "\n",
    "To find out more about NLTK tokenization see:\n",
    "* http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "## Prefix\n",
    "\n",
    "In this notebook we will be creating 4 dummy tweets. This is done to ensure that anyone coming to this notebook is able to test the functionalities of it without needing a set of twitter tweet beforehand. Therefore if you do have a corpus already feel free to skip this section.\n",
    "\n",
    "### Note:\n",
    "\n",
    "This notebook uses Panda as it primary data structure and CSV as it data file format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0     This is a basic tweet without anything speical\n",
       "1                  This tweet uses #twitter hashtags\n",
       "2                     This is used to reply to @user\n",
       "3  #Combing different @uses of things! and http:/..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the panda dataframe\n",
    "pandaDataFrame = pd.DataFrame({ \n",
    "                                'Text' :[\"This is a basic tweet without anything speical\",\n",
    "                                         \"This tweet uses #twitter hashtags\",\n",
    "                                         \"This is used to reply to @user\",\n",
    "                                         \"#Combing different @uses of things! and http://www.google.ca :D\"]\n",
    "                              })\n",
    "\n",
    "# Lets see what the dataframe look like\n",
    "pandaDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now that we have some data loaded into a Panda dataframe, we can begin tokenization. NLTK offers a varity of methods for tokenization. In this notebook we will look at one particular tokenization NLTK offers, that is TweetTokenizer. This is NLTK tokenization made especially for tweets.\n",
    "\n",
    "We will first create a function to conduct the tokenization. The reason for this will be explained later on the notebook. \n",
    "\n",
    "### Note: \n",
    "During tokenization we also record the amount of tokens created. Although not important to this particular notebook, having the count can be useful for analysis.\n",
    "\n",
    "There is also additional options that can be enabled with NLTK TweetTokenizer, however in this notebook we will not be using them.\n",
    "\n",
    "In addition, depending on your data set this process can take a long time. Therefore it is best to segement larger dataset into smaller subsets. The process of segenting larger dataset into smaller ones can be found in a more advance notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenize the text within the dataframe\n",
    "\n",
    ":dataframe: The dataframe with the tweets\n",
    ":column_name: the column name with the tweet text\n",
    "\"\"\"\n",
    "def tokenize_text(dataframe, column_name):\n",
    "\n",
    "    # Initalize the tokenizer\n",
    "    tweetTokenizer = TweetTokenizer()\n",
    "    \n",
    "    # Calculate the amount of tokens\n",
    "    token = tweetTokenizer.tokenize(dataframe[column_name]) \n",
    "\n",
    "    # Save Tokenized text into a new column called \"token\"\n",
    "    dataframe['token'] = token\n",
    "    \n",
    "    # Save Token Count into a new column called \"count_token\"\n",
    "    dataframe['count_token'] = len(token)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it initalized we can apply this to each value of the dataframe. To do this we will examine a nice feature of Panda, \"apply\" and \"lamda\". Using \"apply\" it allows us to apply the function to each data cell in Text. \"Lamda\" allows us to run functions with mulitple parameter in apply.\n",
    "\n",
    "In the \"apply\" function, \"axis\" can either refer to '0' or rows or '1' or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>token</th>\n",
       "      <th>count_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "      <td>[This, is, a, basic, tweet, without, anything,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "      <td>[This, tweet, uses, #twitter, hashtags]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "      <td>[This, is, used, to, reply, to, @user]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "      <td>[#Combing, different, @uses, of, things, !, an...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     This is a basic tweet without anything speical   \n",
       "1                  This tweet uses #twitter hashtags   \n",
       "2                     This is used to reply to @user   \n",
       "3  #Combing different @uses of things! and http:/...   \n",
       "\n",
       "                                               token  count_token  \n",
       "0  [This, is, a, basic, tweet, without, anything,...            8  \n",
       "1            [This, tweet, uses, #twitter, hashtags]            5  \n",
       "2             [This, is, used, to, reply, to, @user]            7  \n",
       "3  [#Combing, different, @uses, of, things, !, an...            9  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the tokenizer on our dataframe and save it into a new dataframe\n",
    "# In this case \"x\" refers to the \"pandaDataframe\" and \"Text\" is the column label\n",
    "TokenizeTweetFrame = pandaDataFrame.apply(lambda x: tokenize_text(x, \"Text\"), axis=1)\n",
    "\n",
    "# Seeing what TokenizeTweetFrame look like\n",
    "TokenizeTweetFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we now have 3 columns in our dataframe (excluding index). You can see that for each text, hashtags, replies, words, and emoji are seperated into their own individual tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting Token\n",
    "\n",
    "Now that we have each tweet tokenized, we can move onto extraction. Although having the tokenized version of each tweet is useful to have, maybe you are only convern with one particular aspect of the tweet text. For example, hashtags are a interesting area to analyze. Therefore having another column with just the hashtags will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Hashtags from Tokens\n",
    "\n",
    "We will first start out by extracting hashtags into their own column. To do so we are going to have to use some regular expression or regex. Regular expression is a method of matching a sequence of characters, in this case hashtags. Therefore we will again declare a function, similar to the one before for tokenization, that extracts hashtags and their count. \n",
    "\n",
    "To find out more on regular expression see:\n",
    "* https://en.wikipedia.org/wiki/Regular_expression\n",
    "\n",
    "For a quick reference on what regular expression can do see:\n",
    "* https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-referencete:\n",
    "\n",
    "### Note:\n",
    "During extraction of the hashtags, we will be removing the \"#\" from the results. This is because we know all the results in the new column will be hashtags, therefore it will be easier and cleaner to remove them during this process than later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the hashtags within the tokenized text\n",
    "\n",
    ":dataframe: The dataframe with the tweets\n",
    ":column_name: the column name with the token text\n",
    "\"\"\"\n",
    "def extract_hashtag(dataframe, column_name):\n",
    "    \n",
    "    # Finds all the hashtags with regex \n",
    "    # The regex matches all sequences that start with \"#\" \n",
    "    hashtag = re.findall(r\"#(\\S+)\", dataframe[column_name])\n",
    "\n",
    "    # Insert hashtag and count into dataframe\n",
    "    \n",
    "    # If there is any hashtag insert them and their count\n",
    "    if hashtag:\n",
    "        dataframe['HASHTAG'] = hashtag\n",
    "        dataframe['count_hashtag'] = len(hashtag)\n",
    "        \n",
    "    # If there is no hashtags just insert empty values\n",
    "    else:\n",
    "        dataframe['HASHTAG'] = []\n",
    "        dataframe['count_hashtag'] = 0\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, similar to before we will again be using Panda's apply and lamda features to apply this function to dataset. In addition we will be expanding the previous dataframe \"TokenizeTweetFrame\" with the new columns. However, feel free to create a new dataframe for the extracted tokens and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>token</th>\n",
       "      <th>count_token</th>\n",
       "      <th>HASHTAG</th>\n",
       "      <th>count_hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "      <td>[This, is, a, basic, tweet, without, anything,...</td>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "      <td>[This, tweet, uses, #twitter, hashtags]</td>\n",
       "      <td>5</td>\n",
       "      <td>[twitter]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "      <td>[This, is, used, to, reply, to, @user]</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "      <td>[#Combing, different, @uses, of, things, !, an...</td>\n",
       "      <td>9</td>\n",
       "      <td>[Combing]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     This is a basic tweet without anything speical   \n",
       "1                  This tweet uses #twitter hashtags   \n",
       "2                     This is used to reply to @user   \n",
       "3  #Combing different @uses of things! and http:/...   \n",
       "\n",
       "                                               token  count_token    HASHTAG  \\\n",
       "0  [This, is, a, basic, tweet, without, anything,...            8         []   \n",
       "1            [This, tweet, uses, #twitter, hashtags]            5  [twitter]   \n",
       "2             [This, is, used, to, reply, to, @user]            7         []   \n",
       "3  [#Combing, different, @uses, of, things, !, an...            9  [Combing]   \n",
       "\n",
       "   count_hashtag  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the extractor on our TokenizeTweetFrame\n",
    "# In this case \"x\" refers to the \"TokenizeTweetFrame\" and \"Text\" is the column label\n",
    "TokenizeTweetFrame = TokenizeTweetFrame.apply(lambda x: extract_hashtag(x, \"Text\"), axis=1)\n",
    "\n",
    "# Seeing what TokenizeTweetFrame look like\n",
    "TokenizeTweetFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted the different hashtags from the text, we can do the same for other tokens as well. In this notebook we will showcase how to also extract urls and replies. However in this notebook we will be moving them to their own dataframe. This is done mostly to keep the dataframe small and readable for this notebook format. Feel free to keep it as one dataframe.\n",
    "\n",
    "We will first start by writing a function that removes the replies. You will find it is very similar to the extract_hashtag function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the replies within the tokenized text\n",
    "\n",
    ":dataframe: The dataframe with the tweets\n",
    ":column_name: the column name with the token text\n",
    "\"\"\"\n",
    "def extract_replies(dataframe, column_name):\n",
    "    \n",
    "    # Finds all the replies with regex \n",
    "    # The regex matches all sequences that start with \"@\"\n",
    "    replies = re.findall(r\"@(\\S+)\", dataframe[column_name])\n",
    "    \n",
    "    # If there is any replies insert them and their count\n",
    "    if replies:\n",
    "        dataframe['REPLIES'] = replies\n",
    "        dataframe['count_replies'] = len(replies)\n",
    "        \n",
    "    # If there is no replies just insert empty values\n",
    "    else:\n",
    "        dataframe['REPLIES'] = []\n",
    "        dataframe['count_replies'] = 0\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to know that we will be using the original pandaDataframe and not the TokenizeTweetFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>REPLIES</th>\n",
       "      <th>count_replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "      <td>[user]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "      <td>[uses]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text REPLIES  count_replies\n",
       "0     This is a basic tweet without anything speical      []              0\n",
       "1                  This tweet uses #twitter hashtags      []              0\n",
       "2                     This is used to reply to @user  [user]              1\n",
       "3  #Combing different @uses of things! and http:/...  [uses]              1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the extractor on our pandaDataFrame\n",
    "# In this case \"x\" refers to the \"pandaDataFrame\" and \"Text\" is the column label\n",
    "ExtractTweetFrame = pandaDataFrame.apply(lambda x: extract_replies(x, \"Text\"), axis=1)\n",
    "\n",
    "# Seeing what ExtractTweetFrame look like\n",
    "ExtractTweetFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final type of token we will extract is the URL links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the URLs within the tokenized text\n",
    "\n",
    ":dataframe: The dataframe with the tweets\n",
    ":column_name: the column name with the token text\n",
    "\"\"\"\n",
    "def extract_URL(dataframe, column_name):\n",
    "    \n",
    "    # Finds all the replies with regex \n",
    "    # The regex matches all sequences that start with \"http\" \n",
    "    URL = re.findall(r\"http\\S+\", dataframe[column_name])\n",
    "    \n",
    "    # If there is any replies insert them and their count\n",
    "    if URL:\n",
    "        dataframe['URL'] = URL\n",
    "        dataframe['count_URL'] = len(URL)\n",
    "        \n",
    "    # If there is no replies just insert empty values\n",
    "    else:\n",
    "        dataframe['URL'] = []\n",
    "        dataframe['count_URL'] = 0\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will be expanding ExtractTweetFrame in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>REPLIES</th>\n",
       "      <th>count_replies</th>\n",
       "      <th>URL</th>\n",
       "      <th>count_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "      <td>[user]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "      <td>[uses]</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://www.google.ca]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text REPLIES  count_replies  \\\n",
       "0     This is a basic tweet without anything speical      []              0   \n",
       "1                  This tweet uses #twitter hashtags      []              0   \n",
       "2                     This is used to reply to @user  [user]              1   \n",
       "3  #Combing different @uses of things! and http:/...  [uses]              1   \n",
       "\n",
       "                      URL  count_URL  \n",
       "0                      []          0  \n",
       "1                      []          0  \n",
       "2                      []          0  \n",
       "3  [http://www.google.ca]          1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the extractor on our pandaDataFrame\n",
    "# In this case \"x\" refers to the \"pandaDataFrame\" and \"Text\" is the column label\n",
    "ExtractTweetFrame = ExtractTweetFrame.apply(lambda x: extract_URL(x, \"Text\"), axis=1)\n",
    "\n",
    "# Seeing what ExtractTweetFrame look like\n",
    "ExtractTweetFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we looked at tokenization using NLTK. The library allowed us to seperate the tweet into different components to be extracted out into their own segement. Using Panda apply and lamda features we were able to extract out hashtags, replies, URL, and their counts into their own columns. \n",
    "\n",
    "Although this notebook does not go into detail on how to save these results (these will be explored in a different notebook), it does provide some of the foundational work that may be needed in one's research. Therefore in keeping this notebook for touching on too many topics, features such as cleaning are not discussed in this particular notebook but will be discussed in another.\n",
    "\n",
    "This notebook serves as one of the introductory notebook in the TATR series. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
