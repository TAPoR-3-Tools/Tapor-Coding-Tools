{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Using NLTK\n",
    "\n",
    "In this notebook we will be looking at tokenziation using nltk (Natural Language Toolkit). \n",
    "\n",
    "Tokenization is the process of segmenting / demarcating a string of input characters. The result of this operation is the creation of \"tokens\".\n",
    "\n",
    "For additional information see: https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization \n",
    "\n",
    "## Libraries and Resources used\n",
    "\n",
    "-  Python 3\n",
    "-  nltk\n",
    "\n",
    "## Note:\n",
    "\n",
    "For installation of the necessary resources and libraries, please refer to their respective home pages to find installation steps for your operation system.\n",
    "\n",
    "\n",
    "Written in February 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "We will first import the required libraries for the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the word tokenizer from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import tweet tokenizer from nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Import multi-word tokenizer from nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Import nltk data for sentence tokenization\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "Using nltk we will tokenize a string into a set of \"tokens\" representing each word / punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'an', 'example', 'sentence', 'to', 'showcase', 'how', 'this', 'process', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create the example sentence\n",
    "exampleSentence = \"I am an example sentence to showcase how this process works.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(exampleSentence)\n",
    "\n",
    "# Print the result\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tokenization turned each word / punctuation of the string (sentence) into their own element and stored it into a list. \n",
    "\n",
    "### Note:\n",
    "\n",
    "The resulting tokens are stored in a Python list and can be accessed / manipulated / altered as any other list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Tokenization\n",
    "\n",
    "Now we will look at a different method of tokenization -- nltk tweet tokenization. Although similar to the previous word tokenizer, there are some differences between the two. One difference is that this method catches common emojis, like \" :) \". We will showcase these differences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'tweet', ':)', '#LivingTheTokenDream', '@ImaginaryFriend', '!', '!', '!']\n",
      "['This', 'is', 'a', 'sample', 'tweet', ':)', '#LivingTheTokenDream', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Initalize the tokenizer\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "\n",
    "# Sample sentence\n",
    "tweetString = \"This is a sample tweet :) #LivingTheTokenDream @ImaginaryFriend!!!!\"\n",
    "\n",
    "# Store the results\n",
    "results = tweetTokenizer.tokenize(tweetString)\n",
    "\n",
    "# Print the results\n",
    "print(results)\n",
    "\n",
    "# You can remove mentions initalizing the tokenizer with different parameters\n",
    "nohandleTokenizer = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "# Store the results\n",
    "results_nohandle = nohandleTokenizer.tokenize(tweetString)\n",
    "\n",
    "# Print the results\n",
    "print(results_nohandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tokenizer recognizes certain characteristics that may not appear outside a tweet. Now we will show what happens when you use the word tokenizer for the previous example tweet sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'tweet', ':', ')', '#', 'LivingTheTokenDream', '@', 'ImaginaryFriend', '!', '!', '!', '!']\n",
      "['This', 'is', 'a', 'sample', 'tweet', ':)', '#LivingTheTokenDream', '@ImaginaryFriend', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Sample sentence\n",
    "tweetString = \"This is a sample tweet :) #LivingTheTokenDream @ImaginaryFriend!!!!\"\n",
    "\n",
    "# Word tokenizer\n",
    "wordResult = word_tokenize(tweetString)\n",
    "\n",
    "# Tweet tokenizer\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "tweetResult = tweetTokenizer.tokenize(tweetString)\n",
    "\n",
    "# Print the two results\n",
    "print(wordResult)\n",
    "print(tweetResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above you can see \"#LivingTheTokenDream\" was separated using the word tokenizer, but it wasn't when using the tweet tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Multiword\n",
    "\n",
    "This tokenization method combines different words together, and allows the user to define combinations of the words they want. For example, \"a lot\" can be treated as one single token rather than two. \n",
    "\n",
    "## Note:\n",
    "\n",
    "This requires you to already have the sentences tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'proves', 'that', 'there', 'is', 'a+lot', 'of', 'new+word', 'out', 'there']\n"
     ]
    }
   ],
   "source": [
    "# Initalize the tokenizer by declaring some multiwords that I want to treat as one word\n",
    "# Also declare how the seperate the two words. (in this case \"a\" and \"lot\" becomes \"a+lot\")\n",
    "MultiWordTokenizer = MWETokenizer([('a', 'lot')], separator='+')\n",
    "\n",
    "# Adding additional words post initalization \n",
    "MultiWordTokenizer.add_mwe(('new', 'word'))\n",
    "\n",
    "# Combining the word combinations together\n",
    "testSentence = word_tokenize(\"This proves that there is a lot of new word out there\")\n",
    "\n",
    "# Print the results\n",
    "results = MultiWordTokenizer.tokenize(testSentence)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing by Sentence\n",
    "\n",
    "In the last section we discussed how to tokenize a sentence into its different components. Now we will show how to tokenize a string by sentences instead of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This string will have a varied amount of sentences!!', 'Some will be proper.', 'others will not be, as it did not capitalize the first letter of the sentence']\n"
     ]
    }
   ],
   "source": [
    "# Declare a bunch of sentences\n",
    "largeText = \"This string will have a varied amount of sentences!! \\\n",
    "Some will be proper. others will not be, as it did not capitalize the first letter of the sentence\"\n",
    "\n",
    "# Tokenize using the sentence tokenizer\n",
    "normalSentenceTokenize = sent_tokenize(largeText)\n",
    "\n",
    "# Print Results\n",
    "print(normalSentenceTokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in a text file\n",
    "\n",
    "Now we will provide a short example on how to load a text file for tokenization. This may be important for different applications, as a corpus may be stored in text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'can', 'even', 'load', 'in', 'from', 'a', 'text', 'file', '.', 'And', 'tokenize', 'it', 'too', '!']\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "file = open(\"dummytext.txt\").read()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(file)\n",
    "\n",
    "# Print the results\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have went over some of the methods of tokenization available within the nltk library in order to tokenize words, tweets, and sentences. The nltk library contains a variety of options when it comes to tokenizing text. For additonal information regarding tokenization using nltk, see the nltk documentation at: http://www.nltk.org/api/nltk.tokenize.html \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
