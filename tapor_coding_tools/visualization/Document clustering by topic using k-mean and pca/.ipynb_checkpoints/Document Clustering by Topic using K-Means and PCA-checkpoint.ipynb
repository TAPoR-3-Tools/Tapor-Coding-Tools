{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Clustering by Topic using K-Means and PCA\n",
    "\n",
    "This notebook's aim is to cluster documents based on features gathered from tf-idf vectorization. This is done first cleaning the text, vectorizing tf-idf the content, and then using k-mean to discover the topics of the corpus. Afterwards using Principal Component Analysis to create the distance matrix for visualization. \n",
    "\n",
    "## Library and Resources used\n",
    "\n",
    "-  Python 3\n",
    "-  Natural Language Toolkit \n",
    "-  Panda\n",
    "-  Numpy\n",
    "-  Scikit-learn (Machine Learning in Python)\n",
    "-  SciPy (Open-source Software for Mathematics, Science, and Engineering)\n",
    "-  Matplotlib (Plotting)\n",
    "\n",
    "## Note:\n",
    "\n",
    "For installation of the nessesary resources and libraries refer to their respective home page for installation steps for your operation system.\n",
    "\n",
    "In this tutorial we will be using novels gather from Gutenberg. It is important to note the novel being used are exclusively from Arthur Conan Doyle, Jane Austen, and Charles Dickens. \n",
    "\n",
    "Also the novels have some cleaning done to them. This involes removing the additional notes made by Gutenberg (inlcuding trademarks, notes about the book, branding) from the start and end of each novel.\n",
    "\n",
    "This Notebook is based and expands upon on a pre-existing IPython Notebook https://github.com/brandomr/document_cluster\n",
    "\n",
    "This Notebook is also similar to \"Document Clustering by Topic using K-Means and MDS\" as they share the exact same code until we attempt to visualize the results of the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the requried libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing base libraries\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import glob\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import nltk\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import mpld3\n",
    "\n",
    "# Import Sklearn\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "# Import Remaining libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we can see plots inside Jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Text from Novels\n",
    "\n",
    "The first step is to collect the names of each novel and their content. This is done by going through all the texts in the \"Novels\" folder. It is important to note the order in which we save both the novel's content and name is synchronized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set path to the file with novels\n",
    "path = \"./Novels\"\n",
    "\n",
    "# Save all the titles of the texts\n",
    "textName = []\n",
    "\n",
    "# Save all the content of the texts\n",
    "textContent = []\n",
    "\n",
    "# Go to the directory with all the text files\n",
    "for filename in os.listdir(path):\n",
    "    \n",
    "    # Add the file name and remove the file type (in this case \".txt\")\n",
    "    textName.append(filename[:-4])\n",
    "    \n",
    "    # Open each file and add all the content \n",
    "    with open(path + '/' + filename, \"r\") as file:\n",
    "         fileContent = file.read()\n",
    "\n",
    "    # Add the content of the file\n",
    "    textContent.append(fileContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint\n",
    "\n",
    "This is just a double check to ensure we have equal amount of titles as content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of text titles matches the amount of text content\n"
     ]
    }
   ],
   "source": [
    "#Check the amount of text being analysis equals the amount of text titles we recorded\n",
    "if len(textName) == len(textContent):\n",
    "    print(\"The amount of text titles matches the amount of text content\")\n",
    "else:\n",
    "    print(\"Amount of content and titles do not match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Text\n",
    "\n",
    "Now we begin cleaning the texts. Outside some pre-cleaning as mentioned before. There is additional cleaning to be done. However we must define some cleaning tools\n",
    "\n",
    "The first being stopwords. The second is stemming words. This means \"happening\", and \"happened\" are converted to the same word \"happen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Use snowballer to break words into their roots\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Cleaning Function\n",
    "\n",
    "Given that there is going to be a lot of cleaning done in this tutorial, I wrote them into their own function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize text \n",
    "def tokenize_Text(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    return tokens\n",
    "\n",
    "# Remove Stop words\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stopwords]\n",
    "\n",
    "# Takes tokenized text and removes all proper nouns with nltk tags\n",
    "def strip_properNouns(token):\n",
    "    # Break down each word into thier category for tags\n",
    "    tags = pos_tag(token)\n",
    "    \n",
    "    # Remove all the words with the proper noun tags or possessives or proper noun purals\n",
    "    # You can alter this to remove more tags (search for nltk tag for more options)\n",
    "    removedProperNoun = [word for word,pos in tags if pos != 'NNP' and pos != 'POS' and pos != 'NNPS']\n",
    "    \n",
    "    return removedProperNoun\n",
    "\n",
    "# Tokenize and stem is used to break each token down into their base components. This is done to make it simplier\n",
    "# on the algorthm later\n",
    "def token_stem(tokens):\n",
    "    \n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "\n",
    "# This filter all non-words out and change contractions\n",
    "def filter_word(tokens):\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            \n",
    "            #Check for common contractions and remove them\n",
    "            #You can add additional cleaning methods here as well\n",
    "            if token[0] == \"\\'\" or token == \"wo\" or token == \"n't\":\n",
    "                filtered_tokens.append(remove_Contractions(token))\n",
    "            else:\n",
    "                filtered_tokens.append(token)\n",
    "            \n",
    "    return filtered_tokens\n",
    "\n",
    "# Remove some common contractions\n",
    "def remove_Contractions(token):\n",
    "    \n",
    "    # Replace contraction with their english word counter part\n",
    "    return token.replace(\"wo\", \"will\").replace(\"n't\", \"not\") \\\n",
    "    .replace(\"'s\", \"is\").replace(\"'m\", \"am\") \\\n",
    "    .replace(\"'ll\", \"will\").replace(\"'d\", \"would\") \\\n",
    "    .replace(\"'re\", \"are\").replace(\"'ve\", \"have\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling all the Cleaning Function\n",
    "\n",
    "Now that there exist a wide varity of cleaning functions to use. It is simplier to just call one function. It is here that you can choose what additional cleaning function you would like to add or remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that calls all the functions I want\n",
    "def full_clean(text):\n",
    "    # This can change depending on your needs\n",
    "    return token_stem(remove_stopwords(filter_word(strip_properNouns(tokenize_Text(text)))))\n",
    "\n",
    "# Define this function to just not include the stemming phase\n",
    "# The reason for this is explained later\n",
    "def full_clean_without_stem(text):\n",
    "     return remove_stopwords(filter_word(strip_properNouns(tokenize_Text(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check point\n",
    "\n",
    "This is to check to see if your cleaning works as intended. It is useful to check to see if your cleaning works ahead of time, since depending on your corpora this may take a while to process. Therefore it better to know now than later if your cleaning method works.\n",
    "\n",
    "Note: If you do not like some aspects of the cleaning presented, feel free to remove them in the \"full_clean\" and \"full_clean_without_stem\" functions above and test the results with some samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proper', 'noun', 'get', 'remov']\n",
      "['do', 'i', 'remov', 'number', 'list']\n",
      "['plural', 'like', 'happen', 'multipl']\n",
      "['what', 'happen']\n",
      "['is', 'one', 'happen', 'happen', 'happen']\n"
     ]
    }
   ],
   "source": [
    "# Testing Full clean\n",
    "test_1 = full_clean(\"Did proper nouns get removed Jimmy?\")\n",
    "test_2 = full_clean(\"Do I remove the number 5 from my list?\")\n",
    "test_3 = full_clean(\"Plurals? like what happens with multiples\")\n",
    "test_4 = full_clean(\"What happens to won't, don't, didn't?\")\n",
    "test_5 = full_clean(\"Is Jimmy one of those happening, happened, or happen\")\n",
    "\n",
    "# Printing test\n",
    "print(test_1)\n",
    "print(test_2)\n",
    "print(test_3)\n",
    "print(test_4)\n",
    "print(test_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that words such as \"remove\" became \"remov\". This is because of the nltk stemming package. Later on in the notebook we will create a parallel list that contains the original. Therefore when we convert it back it will return to its spelling in the text. In this example \"remove\". If you are curious on what words get removed where, feel free to experiment with each function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Cleaning to the Text\n",
    "\n",
    "Now that we have the cleaning function, we are going to clean all the text. First we are going to have two parallel list. One that has the original word and another with the stemmed version. This is to reduce the variation of words in the text for analysis. We keep the original word so we can convert them back later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create two seperate list to hold one with just tokenized words and the others stemmed version of those words\n",
    "text_stemmed = []\n",
    "text_tokenized = []\n",
    "\n",
    "# iterate through all the text\n",
    "for work in textContent:\n",
    "\n",
    "    # Store the text that has been stemmed into text stemmed\n",
    "    alltext_stemmed = full_clean(work)\n",
    "    text_stemmed.extend(alltext_stemmed)\n",
    "    \n",
    "    # Store the all the text that has not been stemmed into text tokenized\n",
    "    alltext_tokenized = full_clean_without_stem(work)\n",
    "    text_tokenized.extend(alltext_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Check\n",
    "Lets check the first 10 words in both list and see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['part', 'the', 'the', 'i', 'inclin', 'think', 'said', 'i', 'i', 'remark']\n",
      "['Part', 'The', 'The', 'I', 'inclined', 'think', 'said', 'I', 'I', 'remarked']\n"
     ]
    }
   ],
   "source": [
    "print(text_stemmed[:10])\n",
    "print(text_tokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to do additional cleaning on the resulting list afterwards. In this example 'the\" reappears. This is because the cleaning process does not remove \"the\" if it appears at the start of the sentence. \n",
    "\n",
    "Remember that any additional cleaning done here should be mirrored in both list. This is ensure that both list are the same length and words line up in their index. If they do not it will cause errors later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Panda Dataframe\n",
    "\n",
    "Now that we have the list it is time to put them into a panda dataframe. Panda is tool that help with data analysis and manipulation. To find out more see their website: https://pandas.pydata.org/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a panda dataframe that has stemmed words as index and tokenized words as columns\n",
    "# This is so that words like \"happened, happening\" are all mapped to the same index \"happen\"\n",
    "vocab_frame = pd.DataFrame({'words': text_tokenized}, index = text_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>part</th>\n",
       "      <td>Part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inclin</th>\n",
       "      <td>inclined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remark</th>\n",
       "      <td>remarked</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words\n",
       "part        Part\n",
       "the          The\n",
       "the          The\n",
       "i              I\n",
       "inclin  inclined\n",
       "think      think\n",
       "said        said\n",
       "i              I\n",
       "i              I\n",
       "remark  remarked"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see the stem words (on the left) matches their original word (right)\n",
    "vocab_frame[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "\n",
    "TF-IDF stands for term frequencyâ€“inverse document frequency. It is a statistic tries determine how important a word is to a document in a collection or corpus.\n",
    "\n",
    "To find out a bit more detail on TF-IDF look at their wikipedia page (https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "The Vectorizer converts the raw data into TF-IDF features. To find out more visit sklearn page (http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 33215)\n"
     ]
    }
   ],
   "source": [
    "# max_df: max cut off for how frequent a term appears in the collection \n",
    "# If it is too frequent it probably holds little meaning\n",
    "\n",
    "# min_df: min start point to be considered a feature\n",
    "# If it appears too infrequently it is probably doesn't have enough importance.\n",
    "\n",
    "# max_features: maximum amount of features that can exist\n",
    "\n",
    "# Tokenizer: gave the previously defined cleaner\n",
    "\n",
    "# ngram_range: Declare that I want to look at unigrams, bigrams and trigrams\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, \n",
    "                                 use_idf=True, tokenizer=full_clean, ngram_range=(1,3))\n",
    "\n",
    "# Fit the vectorizer to the textContent\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(textContent)\n",
    "\n",
    "# See the shape of the vector\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquring Feature Names\n",
    "\n",
    "Now it is time to acquire the terms in the tfidf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save all the terms from vectorizer into a variable to be used later\n",
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Cosine Similarity \n",
    "\n",
    "Now that we have all features and tf-idf of each document it is time to compute the similarity. This is done using cosine similarity which calculates the similarity of a normalized dot product of X and Y. The reason we have 1 subtract the similarity is because we are plotting on a euclidean (2-dimensional) plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity on the tf-idf matrix\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with K-means \n",
    "\n",
    "Now that we have the similarities we are going to cluster the documents using K-means.\n",
    "K-means first asks for a number of clusters. You can have more or less, in this example there is 5. Now K-means attempts to cluster the document in 5 clusters. This process may take a while.\n",
    "\n",
    "To find out more (https://en.wikipedia.org/wiki/K-means_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare amount of clusters\n",
    "num_clusters = 5\n",
    "\n",
    "# Initalize KMeans with the amount of clusters declared\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# Run Kmean on the matrix\n",
    "km.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'km' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8990f8ba455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the cluster that each document belongs to into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'km' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the cluster that each document belongs to into a list\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting The Results For Plotting \n",
    "\n",
    "Now that we have all our results we begin to structure it into a format we can display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the book dictionary to hold the book's title, content, and cluster it belongs to\n",
    "books = { 'title': textName, 'content': textContent, 'cluster': clusters }\n",
    "\n",
    "# Set the panda dataframe\n",
    "# Data is the content of the dictionary\n",
    "# Index is the clusters\n",
    "# Columns as the keys of the dictionary\n",
    "resultFrame = pd.DataFrame(books, index = [clusters] , columns = ['title', 'cluster', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What the dataframe looks like\n",
    "resultFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Have a look at how many documents are in each cluster\n",
    "resultFrame['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking At the Results\n",
    "\n",
    "Before we plot lets have a more detailed look at what documents are in each category and what features that each cluster contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "order_centroids[:, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(vocab_frame)\n",
    "#vocab_frame['russel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster: \\n\")\n",
    "\n",
    "# Cluster the centers by how close they are to the centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# Store the clustered terams\n",
    "clusterTerm = []\n",
    "\n",
    "# For the amount of clusters\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "\n",
    "    term = ''\n",
    "    # Get the top 6 words index in each cluster \n",
    "    for ind in order_centroids[i, :6]:\n",
    "        \n",
    "        # Search the vocab_frame (the dictionary with the original non-stem words)\n",
    "        # Save that into title\n",
    "        title = vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0]\n",
    "        \n",
    "        # Try printing the title\n",
    "        try:\n",
    "            print(\" \" + title + \",\")\n",
    "            term = term + title + \", \"\n",
    "        # If it is unable to print the word found at the index \n",
    "        except:\n",
    "            print(\" \" + vocab_frame.index[ind] + \",\")\n",
    "            term = term + vocab_frame.index[ind] + \", \"\n",
    "\n",
    "    # Begin pronting the titles of each cluster\n",
    "    print(\"\\n\")\n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "\n",
    "    # Print all the titles \n",
    "    try:\n",
    "        for title in resultFrame.ix[i]['title'].values.tolist():\n",
    "            print(' %s,' % title, end='')\n",
    "    except:\n",
    "        print (' %s' % resultFrame.ix[i]['title'], end = '')\n",
    "        \n",
    "    # Append the terms (this is to be used later when we display words of each cluster)\n",
    "    clusterTerm.append(term)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "There are different choices out there to convert the distance matric into a 2-D array. Therefore this notebook will look at Principal Component Analysis to conduct this conversion. \n",
    "\n",
    "Principal Component Analysis tries to identify a smaller number of uncorrelated variables, called \"principal components\" from a dataset. The goal is to explain the maximum amount of variance with the fewest number of principal components.\n",
    "\n",
    "Therefore to find out more on either see the respective links: https://en.wikipedia.org/wiki/Principal_component_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# two components as we're plotting points in a two-dimensional plane\n",
    "# \"full\" because we are using the full singular value decomposition\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "pca = PCA(n_components=2, svd_solver='full', random_state=1)\n",
    "\n",
    "# Fit the model to the distance matrix\n",
    "pos = pca.fit_transform(dist)\n",
    "\n",
    "# Save the results\n",
    "PCAxs, PCAys = pos[:, 0], pos[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setting up Graph Information\n",
    "\n",
    "Now that we completed the data gathering and process portion we move onto declaring some variables to graph. The first step is to set up colours to seperate the categories and their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4:'#ff0000'}\n",
    "\n",
    "cluster_names = {}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: clusterTerm[0], \n",
    "                 1: clusterTerm[1], \n",
    "                 2: clusterTerm[2], \n",
    "                 3: clusterTerm[3], \n",
    "                 4: clusterTerm[4]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing Graph\n",
    "\n",
    "Since we have converted the distance using two different metrics (MSD and PCA) we are going to create a new function that going to plot both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_graph(xs, ys):\n",
    "\n",
    "    #create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "    df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=textName)) \n",
    "\n",
    "    #group by cluster\n",
    "    groups = df.groupby('label')\n",
    "\n",
    "    # set up plot\n",
    "    fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "\n",
    "    #iterate through groups to layer the plot\n",
    "    #note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "    for name, group in groups:\n",
    "        ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "        ax.set_aspect('auto')\n",
    "        ax.tick_params(\\\n",
    "            axis= 'x',         # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            bottom='off',      # ticks along the bottom edge are off\n",
    "            top='off',         # ticks along the top edge are off\n",
    "            labelbottom='off')\n",
    "        ax.tick_params(\\\n",
    "            axis= 'y',         # changes apply to the y-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            left='off',        # ticks along the bottom edge are off\n",
    "            top='off',         # ticks along the top edge are off\n",
    "            labelleft='off')\n",
    "\n",
    "     #show legend with only 1 point\n",
    "    ax.legend(numpoints=1)\n",
    "\n",
    "    #add label to x,y position for the book titles \n",
    "    for i in range(len(df)):\n",
    "        ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    # Display graph\n",
    "    plt.show()\n",
    "\n",
    "    #If you want to save the plot created uncomment the line below and replace \"_insertName_\" with a filename \n",
    "    #plt.savefig('_insertName_.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph PCA\n",
    "create_graph(PCAxs,PCAys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph with A Dendrogram\n",
    "\n",
    "We could also gain additonal insight into the clusters by graphing the results from our distance matrix on a dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a linkage_matrix using ward clustering pre-computed distances\n",
    "linkage_matrix = ward(dist) \n",
    "\n",
    "# Set size of the graph\n",
    "fig, ax = plt.subplots(figsize=(15, 20))\n",
    "\n",
    "# Set the graph type and other labels\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=textName);\n",
    "\n",
    "plt.tick_params(\n",
    "    axis= 'x',         # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "# Display Matrix\n",
    "plt.show()\n",
    "\n",
    "# To save figure, uncomment the line below and replace \"_insertName_\" with the filename\n",
    "#plt.savefig('_insertName'.png', dpi=200) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook went over a couple of different topics. How to clean a text, clustering the documents based on tf-idf, and finally using PCA to convert the maxtrix to visualize the clustering.\n",
    "\n",
    "This Notebook was created, modified, and expanded from Brandon IPython Notebook. They can be found here:\n",
    "https://github.com/brandomr/document_cluster and\n",
    "http://brandonrose.org/clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
