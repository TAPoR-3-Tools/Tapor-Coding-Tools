{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting text\n",
    "In this sample, texts are three ebooks with different topics from Guternburg:\n",
    "1. Adrift in New York (children fiction)\n",
    "2. Beethoven (music)\n",
    "3. Sandwiches (cook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define a function to get .txt files in a folder\n",
    "import codecs\n",
    "from os import listdir\n",
    "def list_textfiles(directory):\n",
    "    \"Return a list of filenames ending in '.txt' in DIRECTORY.\"\n",
    "    textfiles = []\n",
    "    for filename in listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            textfiles.append(directory + \"/\" + filename)\n",
    "    return textfiles   \n",
    "\n",
    "# define a function to read the text in a .txt file\n",
    "\n",
    "def read_txt(filename):\n",
    "    try:\n",
    "        f = codecs.open(filename,'r','utf-8') #open(filename,'r')\n",
    "        text = f.read()\n",
    "    finally:\n",
    "        if f:\n",
    "            f.close()\n",
    "    return text\n",
    "\n",
    "#import harry potter textfiles\n",
    "filenames = list_textfiles('HP')\n",
    "raw_texts = []\n",
    "for n in filenames:\n",
    "    raw_texts.append(read_txt(n))\n",
    "print len(raw_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "- remove stopwords\n",
    "- remove puctuation\n",
    "- lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "clean_texts = []\n",
    "for text in raw_texts:\n",
    "    # tokenize\n",
    "    tok = \" \".join(word_tokenize(text))\n",
    "   \n",
    "    #remove punctuation\n",
    "    punctuation = set(string.punctuation) \n",
    "    re_punc = \"\".join(i for i in tok if i not in punctuation)\n",
    "    \n",
    "    #remove stopwords\n",
    "    re_sw = \" \".join([i for i in re_punc.lower().split() if i not in stopwords.words('english')])\n",
    "    \n",
    "    #lemmatization\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    le = \" \".join(lemmatize.lemmatize(i) for i in re_sw.split())\n",
    "    \n",
    "    clean_texts.append(le)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectorize text\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(min_df = 2,\n",
    "                                strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(clean_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.87193335e-06   7.87369249e-06   8.06729861e-06   9.99290094e-01\n",
      "    6.86092759e-04]\n",
      " [  7.11656199e-06   7.11824747e-06   7.29374521e-06   9.96424926e-01\n",
      "    3.55354498e-03]\n",
      " [  5.68256669e-06   5.68366493e-06   5.85592382e-05   9.98097155e-01\n",
      "    1.83291943e-03]\n",
      " [  3.22608675e-06   3.22680054e-06   2.19777747e-04   6.14371954e-02\n",
      "    9.38336574e-01]\n",
      " [  2.45571321e-06   2.45626786e-06   9.85956933e-01   1.08192770e-02\n",
      "    3.21887795e-03]\n",
      " [  3.88224397e-06   3.88306597e-06   2.26412518e-01   7.64773370e-01\n",
      "    8.80634643e-03]\n",
      " [  3.39798237e-06   3.39870217e-06   9.47496093e-01   4.30811361e-02\n",
      "    9.41597420e-03]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topic = 5\n",
    "lda = LatentDirichletAllocation(n_components = n_topic, \n",
    "                                learning_method='online',\n",
    "                                max_iter=50,\n",
    "                                random_state=0)\n",
    "doctopic = lda.fit(tf)\n",
    "\n",
    "# topic_distribution is a distribution of the topics in each text\n",
    "topic_distribution = lda.transform(tf)\n",
    "\n",
    "print topic_distribution # not normalized (sum of each row is not 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The list above is a probability of topic distrubition in the three texts.\n",
    "\n",
    "- Next we will try to visualize the topic distribution in a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11494c050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ8AAAFpCAYAAACCgq15AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADj9JREFUeJzt3X2MZXddx/H3p13U1a3COiPBlOkiVglP2dbBYDRQsTEm\n4IbYIiVpZBJ1g1AhVB4kEFJiTBQS9I8lgY0koxAoAZWs8pQGKAiRlqWs9MFQEEvBB2hLQTcupNSv\nf+ydcJ0Mnen33DtzD7xfyaTnN/fsOd9O2veec+/k3lQVkvRgnbPXA0gaJ+MhqcV4SGoxHpJajIek\nFuMhqcV4SGoxHpJajIekFuMhqWXfbp9waWmpDl1waLdPq5H67E3/vtcjtPzsxT+51yO0feqmT91d\nVcvb7bfr8Th0wSFuuOHG3T6tRupXHnLNXo/Q8sEbrtnrEdr2PeTcL+5kP29bJLUYD0ktxkNSi/GQ\n1GI8JLUYD0ktxkNSi/GQ1GI8JLUYD0ktxkNSi/GQ1GI8JLUYD0ktxkNSi/GQ1GI8JLVsG48kpzet\n15Icm2w/JclNSb6d5PJ5DSlp8Qy98rgTWAPeNnwUSWMy6D1Mq+oOgCT/O5NpJI3GTuKxP8mpqfVB\n4MSc5pE0EjuJx5mqOryxSLIGrD6YkyQ5ChwFWFlZeTB/VNKC2pVXW6rqeFWtVtXq8tK2HwchaQR8\nqVZSy6B4JHlSki8DzwLelOTW2YwladFt+5xHVR3YtF4H1ifbnwTOn8dgkhabty2SWoyHpBbjIanF\neEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbj\nIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyH\npBbjIanFeEhqMR6SWoyHpJZt45Hk9Kb1WpJjk+2rk9yW5DNJPpjkgnkNKmmxDL3y+DSwWlVPBN4F\nvHb4SJLGYFA8qurDVfU/k+UngPOHjyRpDPbtYJ/9SU5NrQ8CJ7bY77eB981kKkkLbyfxOFNVhzcW\nSdaA1ekdklw5+d5TtzpAkqPAUYCVlZXurJIWyOBXW5JcCrwSOFJV39pqn6o6XlWrVbW6vLQ89JSS\nFsBOrjy+qyQXAW8Cfq2qvjqbkSSNwaB4AK8DDgDvTAJwZ1UdGTyVpIW3bTyq6sCm9TqwPtm+dC5T\nSVp4/oappBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbjIanF\neEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpBbj\nIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpJZt45Hk9Kb1WpJjk+3nJbk5yakkH0vy2HkNKmmx\nDL3yeFtVPaGqDgOvBV4/g5kkjcCgeFTVf00tfwSoYeNIGot9O9hnf5JTU+uDwImNRZIXAFcDPwA8\nbbbjSVpUO7nyOFNVhze+gFdPP1hVb6iqRwMvB1611QGSHE1yMsnJu+6+a/jUkvbcLF9tuRZ45lYP\nVNXxqlqtqtXlpeUZnlLSXhkUjyQXTi2fDnxu2DiSxmInz3k8kKuSXArcB9wLPHf4SJLGYNt4VNWB\nTet1YH2y/aK5TCVp4fkbppJajIekFuMhqcV4SGoxHpJajIekFuMhqcV4SGoxHpJajIekFuMhqcV4\nSGoxHpJajIekFuMhqcV4SGoxHpJajIeklqHvYSrN1XtPb/lpHgvvGQ//k70eYe688pDUYjwktRgP\nSS3GQ1KL8ZDUYjwktRgPSS3GQ1KL8ZDUYjwktRgPSS3GQ1KL8ZDUYjwktRgPSS3GQ1KL8ZDUYjwk\ntRgPSS3bxiPJ6U3rtSTHNn3vsiSVZHXWA0paTIOvPJKcB7wIuGH4OJLGYha3LX8E/CnwzRkcS9JI\n7OSjF/YnOTW1PgicAEhyMfDIqnpPkpfOY0BJi2kn8ThTVYc3FknWgNUk5wCvB9a2O0CSo8BRgJWV\nldagkhbLkNuW84DHA9cnuQN4MnBiqydNq+p4Va1W1ery0vKAU0paFO1PjKuqbwBLG+sk1wMvqaqT\nM5hL0oLz9zwktWx75VFVBzat14H1Lfa7ZFZDSVp8XnlIajEeklqMh6QW4yGpxXhIajEeklqMh6QW\n4yGpxXhIajEeklqMh6QW4yGpxXhIajEeklqMh6QW4yGpxXhIajEeklrab4Cs8bj0h16z1yO0XXfm\n1Xs9Qsvff+UP93qEtn0PeeWO9vPKQ1KL8ZDUYjwktRgPSS3GQ1KL8ZDUYjwktRgPSS3GQ1KL8ZDU\nYjwktRgPSS3GQ1KL8ZDUYjwktRgPSS3GQ1KL8ZDUsm08kpzetF5Lcmxq+64kpyZfvzOvQSUtllm8\nh+k7quqqGRxH0oh42yKpZSdXHvuTnJpaHwROTK0vS/JU4LPAi6vqS7McUNJi2smVx5mqOrzxBUy/\nF/7fAYeq6gnAdcBfbnWAJEeTnExy8q677xo+taQ9N+i2paruqapvTZZ/Afzcd9nveFWtVtXq8tLy\nkFNKWhCD4pHkEVPLI8A/DxtH0lgMfbXlhUmOAN8GvgasDZ5I0ihsG4+qOrBpvQ6sT7ZfAbxiHoNJ\nWmy+VCupxXhIajEeklqMh6QW4yGpxXhIajEeklqMh6QW4yGpxXhIajEeklqMh6QW4yGpxXhIajEe\nklqMh6QW4yGpxXhIapnFJ8Zpwb359hfu9QhtV/3GW/d6hJZjf3PlXo8wd155SGoxHpJajIekFuMh\nqcV4SGoxHpJajIekFuMhqcV4SGoxHpJajIekFuMhqcV4SGoxHpJajIekFuMhqcV4SGoxHpJajIek\nlm3jkeT0pvVakmNT699McluSW5O8bR5DSlo8g94AOcmFwCuAX6yqe5P8xGzGkrToht62/C7whqq6\nF6Cqvjp8JEljsJMrj/1JTk2tDwInJts/A5Dk48C5wDVV9f7ZjihpEe0kHmeq6vDGIskasDr15y8E\nLgHOBz6a5AlV9fXpAyQ5ChwFWFlZGT61pD039Lbly8CJqrqvqv4VuJ2zMfl/qup4Va1W1ery0vLA\nU0paBEPj8W7OXnWQZImztzFfGHhMSSMw9OMmPwD8apLbgPuBl1bVPcPHkrToto1HVR3YtF4H1ifb\nBVw9+ZL0fcTfMJXUYjwktRgPSS3GQ1KL8ZDUYjwktRgPSS3GQ1KL8ZDUYjwktRgPSS3GQ1KL8ZDU\nYjwktRgPSS3GQ1KL8ZDUYjwktQx9D1ONwPMvfuNej9D27v942V6PoO/CKw9JLcZDUovxkNRiPCS1\nGA9JLcZDUovxkNRiPCS1GA9JLcZDUovxkNRiPCS1GA9JLcZDUovxkNRiPCS1GA9JLcZDUovxkNSy\nbTySnN60XktybLL9Z0lOTb5uT/L1eQ0qabEMegPkqnrxxnaS3wcuGjyRpFGY5W3Lc4C3z/B4khbY\nTq489ic5NbU+CJyY3iHJBcCjgA/NcDZJC2wn8ThTVYc3FknWgNVN+1wBvKuq7t/qAEmOAkcBVlZW\nepNKWiizum25gge4Zamq41W1WlWry0vLMzqlpL00OB5JHgM8DPjH4eNIGotZXHlcAVxbVTWDY0ka\niW2f86iqA5vW68D61PqaWQ8lafH5G6aSWoyHpBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyH\npBbjIanFeEhqMR6SWoyHpBbjIanFeEhqMR6SWoyHpJbs9luPJrkL+OKcDr8E3D2nY8+bs+++sc4N\n8539gqra9mMOdj0e85TkZFVt/kyZUXD23TfWuWExZve2RVKL8ZDU8r0Wj+N7PcAAzr77xjo3LMDs\n31PPeUjaPd9rVx6SdsnCxSPJQ5M8f8Cff2+Shz7A4z+Y5B1JPp/khiSHuufa4tjznv0pSW5K8u0k\nl3fPs8Vx5z331UluS/KZJB9MckH3XFsce96zPy/JzUlOJflYksd2z7XFsec6+9R+lyWpJLN9daaq\nFuoLOATcMsfjPx9442T7CuAdI5r9EPBE4K+Ay0c09y8DPzzZ/r2R/cx/dGr7CPD+scw+Ocd5wEeB\nTwCrMz32PAdv/steC5wBTgGvm3zdAtwMPHuyzyWTH8h7gM8CbwTOmTx2B7A02f4t4DPAPwFvmXzv\nA8AvTLb3cfYXbTKG2afOsz7jeOzK3JPHLwI+PtLZnwO8b0yzA38OPB24nu+DeBxiUmPgMuA64Fzg\n4cCdwCMmP9BvAj81eey6jf+ZNn6gwOOA26d+uAcn/7wFOH/qfP+ysc+izz51nnXmdOUxz7kn3zsG\nvGpMswMvmPx38iXgwrHMDlwM/PVk+3pmHI+Fe85jk18C3l5V91fVV4CPAE+aPHZjVX2hqu4H3j7Z\nd9rTgHdW1d0AVfW13Rp6Yqyzz23uJFcCq5z9G3Y0s1fVG6rq0cDLgVeNYfYk5wCvB/5gTvMufDwe\nyObXmHf6mvO/AY8ESLIP+DHgnhnOtRPd2fdae+4klwKvBI5U1bdmOtXOzOJnfi3wzBnM8mB1Zj8P\neDxwfZI7gCcDJ2b5pOkixuO/OfsvDvAPwLOTnJtkGXgKcOPksZ9P8qhJYZ8NfGzTcT4EPCvJjwMk\nOTj5/gnguZPty4EP1eS6bgSzz8tc505yEfAmzobjqyOb/cKpfZ4OfG4Ms1fVN6pqqaoOVdUhzj5h\neqSqTs5q+H2zOtCsVNU9ST6e5BbgfXznSaACXlZV/5nkMcAnOXv//NPAh4G/3XScW5P8MfCRJPcD\nnwbWgDcDb0nyeeBrnH3FZRSzJ3nSZN+HAb+e5DVV9bhFn5uztykHgHcmAbizqo4MnXuXZr9qctV0\nH3Av3/mLZwyzz9Uof8M0ySXAS6rqGXs9y4M11tnHOjc4+7ws4m2LpBEY5ZWHpL3nlYekFuMhqcV4\nSGoxHpJajIekFuMhqeX/ALg5kluFx4JlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114372210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "flight_matrix = topic_distribution \n",
    "\n",
    "yLabel = ['H1','H2','H3','H4','H5','H6','H7']\n",
    "xLabel = ['topic0','topic1','topic2','topic3','topic4']\n",
    "\n",
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(len(xLabel)))\n",
    "ax.set_yticks(np.arange(len(yLabel)))\n",
    "\n",
    "ax.set_xticklabels(xLabel)\n",
    "ax.set_yticklabels(yLabel)\n",
    "\n",
    "heatplot = ax.imshow(flight_matrix, cmap='Purples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
