{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing a HTML Page for URLs using Beautiful Soup 4\n",
    "\n",
    "This notebook's aim is to extract and format hyperlinks from a URL into a csv formatted file. This will cover how to read a URL, use Beautiful Soup 4 and Python 3 to extract the hyperlinks, and finally save them in a csv formatted file. \n",
    "\n",
    "## Libraries and Resources used\n",
    "\n",
    "-  Python 3\n",
    "-  Beautiful Soup 4\n",
    "-  Request (A simple HTTP library for Python)\n",
    "\n",
    "## Note:\n",
    "\n",
    "For installation of the nessesary resources and libraries refer to their respective home page for installation steps for your operation system.\n",
    "\n",
    "In this tutorial we will be using a hyperlink from Wikipedia. It is possible when you are viewing this tutorial, there have been changes done to the URL in question. If that is true, this may result in your own implementation of this code to produce different results or throwing an error.\n",
    "\n",
    "Written in September 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the requried libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the desired URL\n",
    "\n",
    "In this example we are going to use a wikipedia page about the Mid-Autumn Festival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using request we can fetch the URL we want\n",
    "webPage = requests.get('https://en.wikipedia.org/wiki/Mid-Autumn_Festival#Mooncakes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Beauitful Soup Object\n",
    "\n",
    "A Beauitful Soup object is a special type of object that allows us to perform a varitity of different operations. But in this tutorial we will not be using all the functionality built into the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we create a soup object using the text from the webPage we declared eariler\n",
    "#'html.parser' is used to tell the library that we are parsing a HTML\n",
    "soup = BeautifulSoup(webPage.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all the 'a' or 'anchor' Tags\n",
    "\n",
    "Now that we have the web page loaded it is time to get all the URLs. To do this we look for the 'a' Tags or the 'anchor' Tags. These contain the URLs we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we declare we ask Beautiful Soup to find all the 'a' Tags in the webpage\n",
    "anchor_on_page = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the values we want\n",
    "\n",
    "Before we can look at gathering all the URLs we first need to save it somewhere (so later on we have a more organized dataset to work off of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with information about each link\n",
    "linkDictionary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering the URLs\n",
    "\n",
    "Now we are going to examine each 'a' or 'anchor' tag for the keyword 'href'. In HTML this is use to denote a link to another webpage. Therefore we can skip any tags that don't have this key word and save the ones who do have it into the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate through each anchor in our list of anchor\n",
    "for element in anchor_on_page:\n",
    "    \n",
    "    # If the element does not have the keyword we are looking for we are going to skip it\n",
    "    if element.get('href') == None:\n",
    "        continue\n",
    "    \n",
    "    # Create another nested dictionary for each link under \"href\". \n",
    "    # The nested dictionary is used to store another other meta data/ information that is also inside the anchor\n",
    "    linkDictionary[element.get('href')] = {}\n",
    "    \n",
    "    # In the entry for the nested dictionary we are going to assign the keyword \"link\" with the URL link\n",
    "    linkDictionary[element.get('href')]['link'] = str(element.get('href'))\n",
    "    \n",
    "    # Grab any string attached to the anchor. Setting all that have none present to 'null'\n",
    "    if (element.string == None):\n",
    "        linkDictionary[element.get('href')]['string'] = 'null'\n",
    "    else:\n",
    "        linkDictionary[element.get('href')]['string'] = element.string\n",
    "    \n",
    "    # This next part is dependant on the url you are using. In this example, the Wikipedia page sometimes uses the \n",
    "    # word \"title\" inside their anchor. Therefore in the cases that it does appear I want to save it, if it doesn't\n",
    "    # exist then I want to enter \"null\" for the value. \n",
    "    if (element.get('title')) == None:\n",
    "        linkDictionary[element.get('href')]['title'] = 'null'\n",
    "    else:\n",
    "        linkDictionary[element.get('href')]['title'] = element.get('title')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main reasons we don't skip inserting the information of 'title' or 'string' into the dictionary when there isn't one is because we want our csv to be consistence. That each entry will have something in each field, even if it is empty.\n",
    "\n",
    "Depending on the website you are trying to scrap, other meta data other than 'title'. There could be many more or none at all. It is up to you the extent of information you want to record. To check to see what type of other meta data are embedded inspect the source code of the website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing checkpoint\n",
    "\n",
    "Now before we move onto formatting the newly aquired information we are going to see what linkDictionary looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: #mw-head\n",
      " Value: {'link': '#mw-head', 'string': 'navigation', 'title': 'null'}\n",
      "\n",
      "Key: #p-search\n",
      " Value: {'link': '#p-search', 'string': 'search', 'title': 'null'}\n",
      "\n",
      "Key: /wiki/Tsukimi\n",
      " Value: {'link': '/wiki/Tsukimi', 'string': 'Tsukimi', 'title': 'Tsukimi'}\n",
      "\n",
      "Key: /wiki/Chuseok\n",
      " Value: {'link': '/wiki/Chuseok', 'string': 'Chuseok', 'title': 'Chuseok'}\n",
      "\n",
      "Key: /wiki/File:Mid-Autumn_Festival-beijing.jpg\n",
      " Value: {'link': '/wiki/File:Mid-Autumn_Festival-beijing.jpg', 'string': 'null', 'title': 'null'}\n",
      "\n",
      "Key: /wiki/Beijing\n",
      " Value: {'link': '/wiki/Beijing', 'string': 'Beijing', 'title': 'Beijing'}\n",
      "\n",
      "Key: /wiki/China\n",
      " Value: {'link': '/wiki/China', 'string': 'null', 'title': 'China'}\n",
      "\n",
      "Key: /wiki/Taiwan\n",
      " Value: {'link': '/wiki/Taiwan', 'string': 'Taiwan', 'title': 'Taiwan'}\n",
      "\n",
      "Key: /wiki/Malaysia\n",
      " Value: {'link': '/wiki/Malaysia', 'string': 'Malaysia', 'title': 'Malaysia'}\n",
      "\n",
      "Key: /wiki/Singapore\n",
      " Value: {'link': '/wiki/Singapore', 'string': 'Singapore', 'title': 'Singapore'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For each key and value inside the dictionary we print out the result\n",
    "\n",
    "#Limit it to 10 results\n",
    "amountToPrint = 10\n",
    "\n",
    "for key, value in linkDictionary.items():\n",
    "    if amountToPrint > 0:\n",
    "        print(\"Key: \" + str(key) + \"\\n Value: \" + str(value) + \"\\n\")\n",
    "        amountToPrint = amountToPrint - 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see each entry has 3 fields. 'link','string', and 'title' with the key being the link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to a csv file\n",
    "\n",
    "Now that we have all the information we need, it is time to write it into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Open a csv file to save it into (if it doesn't exist it will create one)\n",
    "outputfile = open( 'webPageLink.csv', 'w' )\n",
    "\n",
    "#Iterate through the created dictionary and create an entry for each\n",
    "for key, nestDictionary in  linkDictionary.items() :\n",
    "    #The first feild would be the 'href' value\n",
    "    outputfile.write( str(key))\n",
    "    #For each element in the nested list add it\n",
    "    for value in nestDictionary.values():\n",
    "        outputfile.write(\",\" + str(value))\n",
    "    #End the entry\n",
    "    outputfile.write(\"\\n\")\n",
    "\n",
    "#Close the file\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completition\n",
    "\n",
    "Now you have parsed a website for all the URL redirects on the page and saved all the relavent links, meta data, etc into a csv file. Although in the csv file there is some unnecessary content (in this example, there are some link that take you down to the reference section on the bottom of the web page), every outgoing URL link is now saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Formatting\n",
    "\n",
    "Although the code above gathers all the links, there exist some data that are redirects to some other section of a URL or lack some other information. In the next part we are going to ensure we do not add entries that are not URLs and add back the base URL to some of the entries. To do this we will be modifying the previous import to csv code.\n",
    "\n",
    "In the code we will also be using two helper function. Although their code doesn't have to be their own function, to keep the code a bit cleaner I have opted to have them be their own function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checks the key in our dictionary to see if it is either a new URL or a redirect on the same site\n",
    "def cleanKey(dictionaryKey, baseURL):\n",
    "\n",
    "    #If it is a new url don't change it\n",
    "    #This is done by checking the first 4 character for 'http'\n",
    "    if dictionaryKey[0:4] == 'http':\n",
    "        return dictionaryKey\n",
    "    #If it is a redirect return it with the base site attached to it\n",
    "    #This is done by checking the 1st character and seeing if it is '/'\n",
    "    elif dictionaryKey[0] == '/':\n",
    "        return (baseURL + dictionaryKey)\n",
    "    #If it is not a site return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Does a similar purpose of cleanKey but only cares for redirects\n",
    "def reattachBase(tagWord, baseURL):\n",
    "    if tagWord[0] == '/':\n",
    "        return (baseURL + tagWord)\n",
    "    else:\n",
    "        return tagWord\n",
    "\n",
    "#Write to file called webPageLinkClean\n",
    "outputfile = open( 'webPageLinkClean.csv', 'w' )\n",
    "\n",
    "#The base url of the web page we are scraping (changes based on what site you use)\n",
    "baseURL = \"https://en.wikipedia.org\"\n",
    "    \n",
    "#Iterate through each value in our link dictionary    \n",
    "for key, nestDictionary in  linkDictionary.items() :\n",
    "\n",
    "    #Parse the key\n",
    "    parsedURL = cleanKey(str(key), baseURL)\n",
    "\n",
    "    #If it is not a URL we will add it to the csv\n",
    "    if parsedURL != None:\n",
    "        \n",
    "        #Write into the file the parse/formatted URL\n",
    "        outputfile.write(parsedURL)\n",
    "        \n",
    "        for value in nestDictionary.values():\n",
    "            #If any of the values are a redirect reattach the base\n",
    "            outputfile.write(\",\" + reattachBase(str(value), baseURL))\n",
    "            \n",
    "        outputfile.write(\"\\n\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
