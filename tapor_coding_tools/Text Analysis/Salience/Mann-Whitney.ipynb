{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Using the Mann–Whitney U Test</h1>\n",
    "USES PYTHON 3.x\n",
    "\n",
    "This notebook will run through how to do the <a href=\"https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test\">Mann-Whitney U test</a>, comparing the word usage across two corpora.\n",
    "\n",
    "First we will specify all the directories and filenames that will be used in our code. Keeping them up here makes them easier to find and change later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS1_PATH = \"corpus1/\" # The directory to look in for all the text files to be analyzed.\n",
    "CORPUS2_PATH = \"corpus2/\" # The directory to look in for all the text files to be analyzed.\n",
    "\n",
    "#The names (and locations) of the csv files that will be created:\n",
    "WORD_FREQUENCY_CSV_FILENAME1 = \"Corpus1Frequencies.csv\"\n",
    "WORD_FREQUENCY_CSV_FILENAME2 = \"Corpus2Frequencies.csv\"\n",
    "CORPUS_COMPARISON_FILENAME = \"CorpusCompare.csv\"\n",
    "WORD_LIST_FILENAME = \"words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find All the Files to be Analyzed</h1>\n",
    "Same as with the TF-IDF code, this next section is some very simple code that finds all the text files, and then keeps track of where they are located, so that future functions can find them all when they need to do processing on them. This is cheaper than trying to keep the contents of all the text files in memory. This is a good starting point for any sort of text analysis.\n",
    "\n",
    "One difference from the TF-IDF code is that text files from two different corpora are being tracked, as opposed to text files from just one corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "corpus1dirs = os.listdir(CORPUS1_PATH) # returns list\n",
    "corpus2dirs = os.listdir(CORPUS2_PATH) # returns list\n",
    "corpus1 = []\n",
    "corpus2 = []\n",
    "\n",
    "#Loop over all of the files in the provided directory\n",
    "for file in corpus1dirs:\n",
    "    #Ensure that only text files are included:\n",
    "    if file.endswith(\".txt\"):\n",
    "        text_dir = os.path.join(CORPUS1_PATH, file)\n",
    "        corpus1.append(text_dir)\n",
    "\n",
    "#Loop over all of the files in the provided directory\n",
    "for file in corpus2dirs:\n",
    "    #Ensure that only text files are included:\n",
    "    if file.endswith(\".txt\"):\n",
    "        text_dir = os.path.join(CORPUS2_PATH, file)\n",
    "        corpus2.append(text_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Term Frequency</h1>\n",
    "The Mann-Whitney U test needs some measure to rank by, so the the code below generates Corpus1Frequencies.csv and Corpus2Frequencies.csv files containing the raw word counts for each of the files in both corpora.\n",
    "\n",
    "<b>NOTE:</b> Other measures could be used here (such as relative frequency, for example) and may be better suited to what you're trying to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done first corpus.\n",
      "Done second corpus.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "word_list = []\n",
    "\n",
    "def findFreq(corpus, csvFile):\n",
    "\n",
    "    num_lines = 0\n",
    "    num_words = 0\n",
    "    num_chars = 0\n",
    "\n",
    "    counts = {}\n",
    "    docs = {}\n",
    "    \n",
    "    for text in corpus:\n",
    "        with open(text, 'r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    try:\n",
    "                        counts[word]\n",
    "                    except KeyError:\n",
    "                        counts[word] = 1\n",
    "                    else:\n",
    "                        counts[word] = counts[word] + 1\n",
    "                num_lines += 1\n",
    "                num_words += len(words)\n",
    "                num_chars += len(line)\n",
    "\n",
    "        relativefreqs = {}\n",
    "        for word, rawCount in counts.items():    \n",
    "            # gather only words with alphabetical characters, discard numbers\n",
    "            if word.isalpha():\n",
    "                word_list.append(word)\n",
    "                relativefreqs[word] = rawCount\n",
    "            \n",
    "        # add this document's relative freqs to our dictionary\n",
    "        docs[os.path.basename(text)] = relativefreqs\n",
    "\n",
    "    #output everything to a .csv file, using pandas as a go between.\n",
    "    df = pd.DataFrame(docs)\n",
    "\n",
    "    df.to_csv(csvFile) # write out to CSV\n",
    "\n",
    "findFreq(corpus1, WORD_FREQUENCY_CSV_FILENAME1)\n",
    "print(\"Done first corpus.\")\n",
    "\n",
    "findFreq(corpus2, WORD_FREQUENCY_CSV_FILENAME2)\n",
    "print(\"Done second corpus.\")\n",
    "\n",
    "target = open(WORD_LIST_FILENAME, 'w', encoding=\"utf-8\")\n",
    "\n",
    "unique_words = set(word_list)\n",
    "for word in sorted(unique_words):\n",
    "    target.write(str(word) + \"\\n\")\n",
    "target.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mann–Whitney U Test</h1>\n",
    "The code below then iterates over both of the .csvs generated above, comparing each word across both corpora. If there is a word that only appears in one of the corpora, then a row of zeroes is generated for the corpus that does not contain that word.\n",
    "\n",
    "A .csv file is generated, which shows the ranking for every word, according to whether it appears more in corpus 1 versus corpus 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "df1 = pd.read_csv(WORD_FREQUENCY_CSV_FILENAME1, index_col=0) # read in the CSV\n",
    "df1.rename(columns={'Unnamed: 0': 'Text'}, inplace=True) # add a label to the first column\n",
    "df1 = df1.fillna(0) # replace NaNs with zeroes.\n",
    "\n",
    "df2 = pd.read_csv(WORD_FREQUENCY_CSV_FILENAME2, index_col=0) # read in the CSV\n",
    "df2.rename(columns={'Unnamed: 0': 'Text'}, inplace=True) # add a label to the first column\n",
    "df2 = df2.fillna(0) # replace NaNs with zeroes.\n",
    "\n",
    "# Make \"dummy\" rows of all zeroes for any words that only appear in one corpus and not the other\n",
    "missingInCorpus1 = []\n",
    "missingInCorpus2 = []\n",
    "for i in range(0, df1.shape[1]):\n",
    "    missingInCorpus1.append(0)    \n",
    "\n",
    "for i in range(0, df2.shape[1]):\n",
    "    missingInCorpus2.append(0)\n",
    "\n",
    "# Iterate over the wordlist and the two corpora, and output to csv\n",
    "with open(CORPUS_COMPARISON_FILENAME, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['word', 'Mann Whitney U Value', 'Mann Whitney p-value'])\n",
    "    with open(WORD_LIST_FILENAME, 'r', encoding=\"utf-8\") as f:\n",
    "        for word in f:\n",
    "            word = word.strip()\n",
    "            if (word in df1.index):\n",
    "                countsInCorpus1 = df1.loc[word].values\n",
    "            else:\n",
    "                countsInCorpus1 = missingInCorpus1\n",
    "            if (word in df2.index):\n",
    "                countsInCorpus2 = df2.loc[word].values\n",
    "            else:\n",
    "                countsInCorpus2 = missingInCorpus2\n",
    "            try:\n",
    "                mw = mannwhitneyu(countsInCorpus1, countsInCorpus2)\n",
    "                mwStat = mw.statistic\n",
    "                mwP = mw.pvalue\n",
    "            except ValueError:\n",
    "                mwStat = -1 # in case of ties, Mann-Whitney cannot rank, and so cannot calculate U\n",
    "                mwP = -1\n",
    "            writer.writerow([word, mwStat, mwP])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tables and Graphs</h1>\n",
    "\n",
    "Let's look at a table showing the highest ranked words (likely to be ones that only appear in corpus 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mann Whitney U Value</th>\n",
       "      <th>Mann Whitney p-value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>traincrew</th>\n",
       "      <td>160</td>\n",
       "      <td>2.421461e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utah</th>\n",
       "      <td>160</td>\n",
       "      <td>2.421461e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grunts</th>\n",
       "      <td>160</td>\n",
       "      <td>2.421461e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>broodsows</th>\n",
       "      <td>160</td>\n",
       "      <td>2.421461e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denverthe</th>\n",
       "      <td>160</td>\n",
       "      <td>2.421461e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denvershe</th>\n",
       "      <td>160</td>\n",
       "      <td>2.421461e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>denver</th>\n",
       "      <td>160</td>\n",
       "      <td>1.644058e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dentist</th>\n",
       "      <td>160</td>\n",
       "      <td>1.064701e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quickfooted</th>\n",
       "      <td>160</td>\n",
       "      <td>2.421461e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strapping</th>\n",
       "      <td>160</td>\n",
       "      <td>2.421461e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Mann Whitney U Value  Mann Whitney p-value\n",
       "word                                                   \n",
       "traincrew                     160          2.421461e-07\n",
       "utah                          160          2.421461e-07\n",
       "grunts                        160          2.421461e-07\n",
       "broodsows                     160          2.421461e-07\n",
       "denverthe                     160          2.421461e-07\n",
       "denvershe                     160          2.421461e-07\n",
       "denver                        160          1.644058e-06\n",
       "dentist                       160          1.064701e-05\n",
       "quickfooted                   160          2.421461e-07\n",
       "strapping                     160          2.421461e-07"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CORPUS_COMPARISON_FILENAME, index_col=0) # read in the CSV\n",
    "df2 = df.sort_values(\"Mann Whitney U Value\", ascending=False)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
