{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Using the Mann-Whitney U Test</h1>\n",
    "USES PYTHON 3.x\n",
    "\n",
    "This notebook will run through how to do the <a href=\"https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test\">Mann-Whitney U test</a>, comparing the word usage across two corpora.\n",
    "\n",
    "First we will specify all the directories and filenames that will be used in our code. Keeping them up here makes them easier to find and change later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS1_PATH = \"corpus1/\" # The directory to look in for all the text files to be analyzed.\n",
    "CORPUS2_PATH = \"corpus2/\" # The directory to look in for all the text files to be analyzed.\n",
    "\n",
    "#The names (and locations) of the csv files that will be created:\n",
    "WORD_FREQUENCY_CSV_FILENAME1 = \"Corpus1Frequencies.csv\"\n",
    "WORD_FREQUENCY_CSV_FILENAME2 = \"Corpus2Frequencies.csv\"\n",
    "CORPUS_COMPARISON_FILENAME = \"CorpusCompare.csv\"\n",
    "WORD_LIST_FILENAME = \"words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find All the Files to be Analyzed</h1>\n",
    "Same as with the TF-IDF code, this next section is some very simple code that finds all the text files, and then keeps track of where they are located, so that future functions can find them all when they need to do processing on them. This is cheaper than trying to keep the contents of all the text files in memory. This is a good starting point for any sort of text analysis.\n",
    "\n",
    "One difference from the TF-IDF code is that text files from two different corpora are being tracked, as opposed to text files from just one corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "corpus1dirs = os.listdir(CORPUS1_PATH) # returns list\n",
    "corpus2dirs = os.listdir(CORPUS2_PATH) # returns list\n",
    "corpus1 = []\n",
    "corpus2 = []\n",
    "\n",
    "#Loop over all of the files in the provided directory\n",
    "for file in corpus1dirs:\n",
    "    #Ensure that only text files are included:\n",
    "    if file.endswith(\".txt\"):\n",
    "        text_dir = os.path.join(CORPUS1_PATH, file)\n",
    "        corpus1.append(text_dir)\n",
    "\n",
    "#Loop over all of the files in the provided directory\n",
    "for file in corpus2dirs:\n",
    "    #Ensure that only text files are included:\n",
    "    if file.endswith(\".txt\"):\n",
    "        text_dir = os.path.join(CORPUS2_PATH, file)\n",
    "        corpus2.append(text_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Term Frequency</h1>\n",
    "\n",
    "The Mann-Whitney U test needs some measure to rank by, so the the code below generates Corpus1Frequencies.csv and Corpus2Frequencies.csv files containing the relative frequencies for each word in each of the files in both corpora.\n",
    "\n",
    "<b>NOTE:</b> Other measures could be used here (such as raw frequency, for example) and may be better suited to what you're trying to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done first corpus.\n",
      "Done second corpus.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "word_list = []\n",
    "\n",
    "def findFreq(corpus, csvFile):\n",
    "\n",
    "    texts = []\n",
    "    docs = {}\n",
    "    num_words = 0\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    for text in corpus:\n",
    "        num_words = 0\n",
    "        with open(text, 'r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                # Use Regex to remove punctuation and isolate words\n",
    "                words = re.findall(r'(\\b\\S+\\b|#\\w+|@\\w+)', line.lower())\n",
    "                for word in words:\n",
    "                    counts[word] += 1\n",
    "                num_words += len(words)\n",
    "\n",
    "        relativefreqs = {}\n",
    "        for word, rawCount in counts.items():\n",
    "            word_list.append(word)\n",
    "            relativefreqs[word] = rawCount / float(num_words)\n",
    "            counts[word] = 0\n",
    "        # add this document's relative freqs to our dictionary\n",
    "        docs[os.path.basename(text)] = relativefreqs\n",
    "        #print(\"Done with \" + text)\n",
    "\n",
    "    #output everything to a .csv file, using pandas as a go between.\n",
    "    df = pd.DataFrame(docs)\n",
    "    df = df.fillna(0)\n",
    "    df.to_csv(csvFile, encoding=\"utf-8\") # write out to CSV\n",
    "\n",
    "findFreq(corpus1, WORD_FREQUENCY_CSV_FILENAME1)\n",
    "print(\"Done first corpus.\")\n",
    "\n",
    "findFreq(corpus2, WORD_FREQUENCY_CSV_FILENAME2)\n",
    "print(\"Done second corpus.\")\n",
    "\n",
    "target = open(WORD_LIST_FILENAME, 'w', encoding=\"utf-8\")\n",
    "\n",
    "unique_words = set(word_list)\n",
    "for word in sorted(unique_words):\n",
    "    target.write(str(word) + \"\\n\")\n",
    "target.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mann–Whitney U Test</h1>\n",
    "The code below then iterates over both of the .csvs generated above, comparing each word across both corpora. If there is a word that only appears in one of the corpora, then a row of zeroes is generated for the corpus that does not contain that word.\n",
    "\n",
    "A .csv file is generated, which shows the ranking for every word, according to whether it is more salient to corpus 2 versus corpus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "df1 = pd.read_csv(WORD_FREQUENCY_CSV_FILENAME1, index_col=0) # read in the CSV\n",
    "df1.rename(columns={'Unnamed: 0': 'Text'}, inplace=True) # add a label to the first column\n",
    "df1 = df1.fillna(0) # replace NaNs with zeroes.\n",
    "\n",
    "df2 = pd.read_csv(WORD_FREQUENCY_CSV_FILENAME2, index_col=0) # read in the CSV\n",
    "df2.rename(columns={'Unnamed: 0': 'Text'}, inplace=True) # add a label to the first column\n",
    "df2 = df2.fillna(0) # replace NaNs with zeroes.\n",
    "\n",
    "total_docs = len(df1.columns) * len(df2.columns)\n",
    "\n",
    "# Make \"dummy\" rows of all zeroes for any words that only appear in one corpus and not the other\n",
    "missingInCorpus1 = []\n",
    "missingInCorpus2 = []\n",
    "for i in range(0, df1.shape[1]):\n",
    "    missingInCorpus1.append(0)    \n",
    "\n",
    "for i in range(0, df2.shape[1]):\n",
    "    missingInCorpus2.append(0)\n",
    "\n",
    "# Iterate over the wordlist and the two corpora, and output to csv\n",
    "with open(CORPUS_COMPARISON_FILENAME, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['word', 'Mann Whitney U Value', 'Mann Whitney rho-value'])\n",
    "    with open(WORD_LIST_FILENAME, 'r', encoding=\"utf-8\") as f:\n",
    "        for word in f:\n",
    "            word = word.strip()\n",
    "            if (word in df1.index):\n",
    "                countsInCorpus1 = df1.loc[word].values\n",
    "            else:\n",
    "                countsInCorpus1 = missingInCorpus1\n",
    "            if (word in df2.index):\n",
    "                countsInCorpus2 = df2.loc[word].values\n",
    "            else:\n",
    "                countsInCorpus2 = missingInCorpus2\n",
    "            try:\n",
    "                mw = mannwhitneyu(countsInCorpus1, countsInCorpus2)\n",
    "                mwStat = mw.statistic\n",
    "                mwRho = mwStat / total_docs\n",
    "            except ValueError: # Was having problems with this earlier, so this is mainly for debugging reasons\n",
    "                mwStat = -1\n",
    "                mwRho = -1\n",
    "            writer.writerow([word, mwStat, mwRho])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tables and Graphs</h1>\n",
    "\n",
    "Let's look at a table showing the highest ranked words (likely to be ones that are more salient to corpus 1). Sorted by the Mann-Whitney ρ value, which is just the U rank divided by the total number of documents in each corpus, multiplied together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mann Whitney U Value</th>\n",
       "      <th>Mann Whitney rho-value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sit</th>\n",
       "      <td>147</td>\n",
       "      <td>0.91875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vexed</th>\n",
       "      <td>146</td>\n",
       "      <td>0.91250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curls</th>\n",
       "      <td>146</td>\n",
       "      <td>0.91250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheerful</th>\n",
       "      <td>145</td>\n",
       "      <td>0.90625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usually</th>\n",
       "      <td>143</td>\n",
       "      <td>0.89375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roused</th>\n",
       "      <td>141</td>\n",
       "      <td>0.88125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forgetting</th>\n",
       "      <td>141</td>\n",
       "      <td>0.88125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cousin</th>\n",
       "      <td>140</td>\n",
       "      <td>0.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impatient</th>\n",
       "      <td>140</td>\n",
       "      <td>0.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tying</th>\n",
       "      <td>140</td>\n",
       "      <td>0.87500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mann Whitney U Value  Mann Whitney rho-value\n",
       "word                                                    \n",
       "sit                          147                 0.91875\n",
       "vexed                        146                 0.91250\n",
       "curls                        146                 0.91250\n",
       "cheerful                     145                 0.90625\n",
       "usually                      143                 0.89375\n",
       "roused                       141                 0.88125\n",
       "forgetting                   141                 0.88125\n",
       "cousin                       140                 0.87500\n",
       "impatient                    140                 0.87500\n",
       "tying                        140                 0.87500"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CORPUS_COMPARISON_FILENAME, index_col=0) # read in the CSV\n",
    "df2 = df.sort_values(\"Mann Whitney rho-value\", ascending=False)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
