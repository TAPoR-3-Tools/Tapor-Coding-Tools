{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Using NLTK\n",
    "\n",
    "In this notebook we will be looking at tokenziation using nltk (Natural Language Toolkit). \n",
    "\n",
    "Tokenization is the process of segementing / demarcating a string of input characters. The result of this operation is the creation of \"tokens\".\n",
    "\n",
    "For additional information see: https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization \n",
    "\n",
    "## Libraries and Resources used\n",
    "\n",
    "-  Python 3\n",
    "-  nltk\n",
    "\n",
    "## Note:\n",
    "\n",
    "For installation of the nessesary resources and libraries refer to their respective home page for installation steps for your operation system.\n",
    "\n",
    "\n",
    "Written in February 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "We will first import the required libraries for the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the word tokenizer from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import tweet tokenizer from nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Import multi-word tokenizer from nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Import nltk data for sentence tokenization\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "Using nltk we will tokenize a string into a set of \"tokens\" representing each word / punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'an', 'example', 'sentence', 'to', 'showcase', 'how', 'this', 'process', 'works', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create the example sentence\n",
    "exampleSentence = \"I am an example sentence to showcase how this process works.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(exampleSentence)\n",
    "\n",
    "# Print the result\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the tokenization turned each word / punctuation of the string (sentence) into their own element and stores it into a list. \n",
    "\n",
    "### Note:\n",
    "\n",
    "The resulting tokens are stored in a Python list and can be accessed / manipulated / altered as any other list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Tokenization\n",
    "\n",
    "Now we will look at a different method of tokenization. This time we will look at nltk tweet tokenization. Although similar to the previous word tokenizer, there is some difference between the two. One difference is that this catches common emoji like \":)\". We will showcase some of the differences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'tweet', ':)', '#LivingTheTokenDream', '@ImaginaryFriend', '!', '!', '!']\n",
      "['This', 'is', 'a', 'sample', 'tweet', ':)', '#LivingTheTokenDream', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Initalize the tokenizer\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "\n",
    "# Sample Sentence\n",
    "tweetString = \"This is a sample tweet :) #LivingTheTokenDream @ImaginaryFriend!!!!\"\n",
    "\n",
    "# Store the results\n",
    "results = tweetTokenizer.tokenize(tweetString)\n",
    "\n",
    "# Print the results\n",
    "print(results)\n",
    "\n",
    "# You can remove mentions initalizing the tokenizer with different parameters\n",
    "nohandleTokenizer = TweetTokenizer(strip_handles=True)\n",
    "\n",
    "# Store the results\n",
    "results_nohandle = nohandleTokenizer.tokenize(tweetString)\n",
    "\n",
    "# Print the results\n",
    "print(results_nohandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the tokenizer recognizes certain characteristic that may not appear outside a tweet. We will now showcase what happens when you use the word tokenizer for the previous example tweet sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'tweet', ':', ')', '#', 'LivingTheTokenDream', '@', 'ImaginaryFriend', '!', '!', '!', '!']\n",
      "['This', 'is', 'a', 'sample', 'tweet', ':)', '#LivingTheTokenDream', '@ImaginaryFriend', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "# Sample Sentence\n",
    "tweetString = \"This is a sample tweet :) #LivingTheTokenDream @ImaginaryFriend!!!!\"\n",
    "\n",
    "# Word tokenizer\n",
    "wordResult = word_tokenize(tweetString)\n",
    "\n",
    "# Tweet tokenizer\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "tweetResult = tweetTokenizer.tokenize(tweetString)\n",
    "\n",
    "# Print the two results\n",
    "print(wordResult)\n",
    "print(tweetResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above you can see \"#LivingTheTokenDream\" was seperated using the word tokenizer but it wasn't when using the tweet tokenizer. Showcasing the difference between the different type of tokenizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Multiword\n",
    "\n",
    "This tokenization method combines different words together, except it allows the user to define combinations of words they want. For example, \"a lot\" can be treated as one single token rather than 2. \n",
    "\n",
    "## Note:\n",
    "\n",
    "This requires you to either have the sentences tokenized already in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'proves', 'that', 'there', 'is', 'a+lot', 'of', 'new+word', 'out', 'there']\n"
     ]
    }
   ],
   "source": [
    "# Initalize the tokenizer by declaring some multiwords that I want to treat as one word\n",
    "# Also declare how the seperate the two words. (in this case \"a\" and \"lot\" becomes \"a+lot\")\n",
    "MultiWordTokenizer = MWETokenizer([('a', 'lot')], separator='+')\n",
    "\n",
    "# Adding additional words post initalization \n",
    "MultiWordTokenizer.add_mwe(('new', 'word'))\n",
    "\n",
    "# Combining the word combinations together\n",
    "testSentence = word_tokenize(\"This proves that there is a lot of new word out there\")\n",
    "\n",
    "# Print the results\n",
    "results = MultiWordTokenizer.tokenize(testSentence)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing by Sentence\n",
    "\n",
    "Now that we discussed how to tokenize a sentence into their difference components. We will now discuss how to tokenize a string by sentences instead of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This string will have a varied amount of sentences!!', 'Some will be proper.', 'others will not be, as it did not capitalize the first letter of the sentence']\n"
     ]
    }
   ],
   "source": [
    "# Declare a bunch of sentences\n",
    "largeText = \"This string will have a varied amount of sentences!! \\\n",
    "Some will be proper. others will not be, as it did not capitalize the first letter of the sentence\"\n",
    "\n",
    "# Tokenize using the sentence tokenizer\n",
    "normalSentenceTokenize = sent_tokenize(largeText)\n",
    "\n",
    "# Print Results\n",
    "print(normalSentenceTokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in a text file\n",
    "\n",
    "Now we will showcase a short example on how to load a text file for tokenization. This may be important for different applications as a corpus can be stored in text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'can', 'even', 'load', 'in', 'from', 'a', 'text', 'file', '.', 'And', 'tokenize', 'it', 'too', '!']\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "file = open(\"dummytext.txt\").read()\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(file)\n",
    "\n",
    "# Print the results\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have went over some of the methods of tokenization available within the nltk library. From tokenizing words, tweets, and sentences. nltk proves a varity of option to tokenize a text. For additonal information regarding tokenization using nltk, see nltk documentation at: http://www.nltk.org/api/nltk.tokenize.html \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
