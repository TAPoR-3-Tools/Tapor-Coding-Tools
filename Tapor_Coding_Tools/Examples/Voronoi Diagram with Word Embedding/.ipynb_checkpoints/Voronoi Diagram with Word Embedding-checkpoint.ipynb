{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voronoi Diagram with Word Embedding\n",
    "\n",
    "In this notebook, we will examine how to use word embedding and voronoi diagram to display the relationship between others words and a search term. Using Tale of Two Cities by Charles Dickens as the example (aquired from https://www.gutenberg.org/) we will showcase how the program words as well as explain different modification and steps that can be taken in the future\n",
    "\n",
    "## Libraries and Resources used\n",
    "\n",
    "-  Python 3\n",
    "-  nltk\n",
    "-  scipy\n",
    "-  matplotlib\n",
    "-  re (regex)\n",
    "-  shapely\n",
    "-  numpy \n",
    "-  gemsim\n",
    "\n",
    "## Note:\n",
    "\n",
    "For installation of the nessesary resources and libraries refer to their respective home page for installation steps for your operation system. \n",
    "\n",
    "The text being analysised (Tales of Two Cities by Charles Dicken) has some cleaning done to it beforehand. We have removed the sections that gutenberg documented (copyrights, etc) to simulate a more realistic corpus.\n",
    "\n",
    "Written in Feburary 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the requried libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import libraries for Voronoi Diagram\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Voronoi\n",
    "from shapely.ops import polygonize\n",
    "from shapely.geometry import LineString, MultiPolygon, MultiPoint, Point, Polygon\n",
    "\n",
    "# Import nltk and cleaning libraries\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Vectorization\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Corpus\n",
    "\n",
    "We will start out by loading in the corpus and tokenizing the text by sentence using nltk. Depening on the corpus the result of this may be different (for example using a collection of tweets would result in a vastly different results as this notebook is not tailored for tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff\\n\\nCONTENTS\\n\\n\\n     Book the First--Recalled to Life\\n\\n     Chapter I      The Period\\n     Chapter II     The Mail\\n     Chapter III    The Night Shadows\\n     Chapter IV     The Preparation\\n     Chapter V      The Wine-shop\\n     Chapter VI     The Shoemaker\\n\\n\\n     Book the Second--the Golden Thread\\n\\n     Chapter I      Five Years Later\\n     Chapter II     A Sight\\n     Chapter III    A Disappointment\\n     Chapter IV     Congratulatory\\n     Chapter V      The Jackal\\n     Chapter VI     Hundreds of People\\n     Chapter VII    Monseigneur in Town\\n     Chapter VIII   Monseigneur in the Country\\n     Chapter IX     The Gorgon’s Head\\n     Chapter X      Two Promises\\n     Chapter XI     A Companion Picture\\n     Chapter XII    The Fellow of Delicacy\\n     Chapter XIII   The Fellow of no Delicacy\\n     Chapter XIV    The Honest Tradesman\\n     Chapter XV     Knitting\\n     Chapter XVI    Still Knitting\\n     Chapter XVII   One Night\\n     Chapter XVIII  Nine Days\\n     Chapter XIX    An Opinion\\n     Chapter XX     A Plea\\n     Chapter XXI    Echoing Footsteps\\n     Chapter XXII   The Sea Still Rises\\n     Chapter XXIII  Fire Rises\\n     Chapter XXIV   Drawn to the Loadstone Rock\\n\\n\\n     Book the Third--the Track of a Storm\\n\\n     Chapter I      In Secret\\n     Chapter II     The Grindstone\\n     Chapter III    The Shadow\\n     Chapter IV     Calm in Storm\\n     Chapter V      The Wood-sawyer\\n     Chapter VI     Triumph\\n     Chapter VII    A Knock at the Door\\n     Chapter VIII   A Hand at Cards\\n     Chapter IX     The Game Made\\n     Chapter X      The Substance of the Shadow\\n     Chapter XI     Dusk\\n     Chapter XII    Darkness\\n     Chapter XIII   Fifty-two\\n     Chapter XIV    The Knitting Done\\n     Chapter XV     The Footsteps Die Out For Ever\\n\\n\\n\\n\\n\\nBook the First--Recalled to Life\\n\\n\\n\\n\\nI.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the text file\n",
    "with open(\"A Tale of Two Cities.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    TalesOfTwoCities = f.read()\n",
    "\n",
    "# Tokenize the text by sentence\n",
    "novelTokenSentence = sent_tokenize(TalesOfTwoCities)\n",
    "\n",
    "# Display the first sentence\n",
    "novelTokenSentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text\n",
    "\n",
    "To help with some of the analysis we are going to clean the text. This is done by removing all punication, lowercasing all the words and finally lemmitizing them. This is help streamline and reduce variability when we train the model for word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Helper Functions\n",
    "\n",
    "This function is what cleans the text. In this notebook we first convert the sentence to lowercase, followed by removing all the punctuation and finally lemmatize the remaining words. If there is any additional aspect or modification needed this is where you would add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare a set of stopwords and puncation to remove\n",
    "stopword_punctuation = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Initalize Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Cleaning function that cleans the sentence given to it\n",
    "def cleanText(sentence):\n",
    "    # Converts to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # tokenize the sentence\n",
    "    sentence = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove all the stopwords / punctuation \n",
    "    sentence = filter(lambda token: token not in stopword_punctuation, sentence)\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    sentence = list(map(wordnet_lemmatizer.lemmatize,sentence))\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting the Cleaning\n",
    "\n",
    "Now that we declared the helper function to clean the text it is time to clean the text.\n",
    "\n",
    "#### Note:\n",
    "That we are display the 2nd results. This is because the first result or element in the list is the chapter headings / index. Therefore for this notebook we chose to skip it to showcase an example from the text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cleaning method on each element (sentence) in our novel\n",
    "novelTextClean = list(map(cleanText, novelTokenSentence))\n",
    "\n",
    "# Display 2nd results of the cleaning and the original before cleaning\n",
    "print(\"This is the original: \\n\")\n",
    "print(novelTokenSentence[1])\n",
    "print(\"This is the clean: \\n\")\n",
    "print(novelTextClean[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Collocating Terms\n",
    "\n",
    "Now that we have all the sentences tokenzied and cleaned, we can now do basic collocation. This is done by searching every sentence and finding if a desired word exist in that sentence. If it does we will add it to a new list of results. This is done so we can use word embedding later on to help vecotrize the result. For this example we will be looking for the term \"city\" in the text.\n",
    "\n",
    "### Note:\n",
    "\n",
    "Rememeber that all the tokens in the novel are in lowercase and lemmatized. Take this into account when choosing your search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that find all sentences with the search term\n",
    "def findSentence(word, novel):\n",
    "    resultSet = []\n",
    "    for sentence in novel:\n",
    "        if word in sentence:\n",
    "            resultSet.append(sentence)\n",
    "    return resultSet\n",
    "\n",
    "searchTerm = \"city\"\n",
    "\n",
    "# We will store all the result of the term \"city\" into sentenceWithSearchTerm\n",
    "sentenceWithSearchTerm = findSentence(searchTerm, novelTextClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec is a machine learning model that produces word embeddings. From there we can use Principle Componenet Analysis (PCA) to convert these vectors into a 2d plane for the Voronoi Diagram. For additional information regarding Word2Vec,  word embedding, and PCA see https://en.wikipedia.org/wiki/Word2vec , https://en.wikipedia.org/wiki/Word_embedding, and https://en.wikipedia.org/wiki/Principal_component_analysis respectively. \n",
    "\n",
    "### Note:\n",
    "This section used code that was taken/adapted from https://machinelearningmastery.com/develop-word-embeddings-python-gensim/ \n",
    "\n",
    "Also to reduce the amount of points found, we will only use words that appear more than 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model using our results from earlier with words that appear atleast more than 5 times\n",
    "Word2VecModel = Word2Vec(sentenceWithSearchTerm, min_count=5)\n",
    "\n",
    "# save model for later use \n",
    "Word2VecModel.save('model.bin')\n",
    "\n",
    "# load model\n",
    "Word2VecModel = Word2Vec.load('model.bin')\n",
    "\n",
    "# Store all the words in the model\n",
    "VectorWords = list(Word2VecModel.wv.vocab)\n",
    "\n",
    "# Access all of the vectors from a trained model \n",
    "vectorOfTrainedModel = Word2VecModel[Word2VecModel.wv.vocab]\n",
    "\n",
    "# Initilize PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Transform the data to 2D representation\n",
    "Vector2DResults = pca.fit_transform(vectorOfTrainedModel)\n",
    "\n",
    "# Display results\n",
    "Vector2DResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voronoi Diagram \n",
    "Now that we have the 2D representation of all the word embedding. We can now map them on a Voronoi Diagram. For additional inforamtion on what a voronoi diagram is see: https://en.wikipedia.org/wiki/Voronoi_diagram\n",
    "\n",
    "### Note:\n",
    "To create the voronoi diagram I have taken code from: \n",
    "https://stackoverflow.com/questions/20515554/colorize-voronoi-diagram/20678647#20678647 and\n",
    "https://stackoverflow.com/questions/34968838/python-finite-boundary-voronoi-cells\n",
    "\n",
    "For additional information on modification to the following code please see the respective authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Voronoi Functions\n",
    "\n",
    "Function taken directly from: https://stackoverflow.com/questions/20515554/colorize-voronoi-diagram/20678647#20678647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def voronoi_finite_polygons_2d(vor, radius=None):\n",
    "    \"\"\"\n",
    "    Reconstruct infinite voronoi regions in a 2D diagram to finite\n",
    "    regions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vor : Voronoi\n",
    "        Input diagram\n",
    "    radius : float, optional\n",
    "        Distance to 'points at infinity'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    regions : list of tuples\n",
    "        Indices of vertices in each revised Voronoi regions.\n",
    "    vertices : list of tuples\n",
    "        Coordinates for revised Voronoi vertices. Same as coordinates\n",
    "        of input vertices, with 'points at infinity' appended to the\n",
    "        end.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if vor.points.shape[1] != 2:\n",
    "        raise ValueError(\"Requires 2D input\")\n",
    "\n",
    "    new_regions = []\n",
    "    new_vertices = vor.vertices.tolist()\n",
    "\n",
    "    center = vor.points.mean(axis=0)\n",
    "    if radius is None:\n",
    "        radius = vor.points.ptp().max()\n",
    "\n",
    "    # Construct a map containing all ridges for a given point\n",
    "    all_ridges = {}\n",
    "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
    "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
    "\n",
    "    # Reconstruct infinite regions\n",
    "    for p1, region in enumerate(vor.point_region):\n",
    "        vertices = vor.regions[region]\n",
    "\n",
    "        if all(v >= 0 for v in vertices):\n",
    "            # finite region\n",
    "            new_regions.append(vertices)\n",
    "            continue\n",
    "\n",
    "        # reconstruct a non-finite region\n",
    "        ridges = all_ridges[p1]\n",
    "        new_region = [v for v in vertices if v >= 0]\n",
    "\n",
    "        for p2, v1, v2 in ridges:\n",
    "            if v2 < 0:\n",
    "                v1, v2 = v2, v1\n",
    "            if v1 >= 0:\n",
    "                # finite ridge: already in the region\n",
    "                continue\n",
    "\n",
    "            # Compute the missing endpoint of an infinite ridge\n",
    "\n",
    "            t = vor.points[p2] - vor.points[p1] # tangent\n",
    "            t /= np.linalg.norm(t)\n",
    "            n = np.array([-t[1], t[0]])  # normal\n",
    "\n",
    "            midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
    "            direction = np.sign(np.dot(midpoint - center, n)) * n\n",
    "            far_point = vor.vertices[v2] + direction * radius\n",
    "\n",
    "            new_region.append(len(new_vertices))\n",
    "            new_vertices.append(far_point.tolist())\n",
    "\n",
    "        # sort region counterclockwise\n",
    "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
    "        c = vs.mean(axis=0)\n",
    "        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n",
    "        new_region = np.array(new_region)[np.argsort(angles)]\n",
    "\n",
    "        # finish\n",
    "        new_regions.append(new_region.tolist())\n",
    "\n",
    "    return new_regions, np.asarray(new_vertices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Voronoi Diagram\n",
    "\n",
    "Now that we declared the function to create the voronoi diagram, we need to plot it. Using shapely and matplotlib we now create a clipped Voronoi diagram. The reason for \"clipped\" is to create a more appealing diagram. There are case where the resulting voronoi diagram extends it borders to vastly empty region making the visulation less useful. Therefore we will attempt to trim down the borders of the diagram.\n",
    "\n",
    "### Note:\n",
    "The following code is adapted and taken from: https://stackoverflow.com/questions/34968838/python-finite-boundary-voronoi-cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Voronoi from teh results\n",
    "vor = Voronoi(Vector2DResults)\n",
    "\n",
    "# plot the results of the voronoi \n",
    "regions, vertices = voronoi_finite_polygons_2d(vor)\n",
    "\n",
    "# Create the point of the results\n",
    "pts = MultiPoint([Point(i) for i in Vector2DResults])\n",
    "\n",
    "# Create a mask for the ppoints\n",
    "mask = pts.convex_hull.union(pts.buffer(10, resolution=50, cap_style=2))\n",
    "\n",
    "# Creation of the new vertices and plotting the voronoi diagram\n",
    "new_vertices = []\n",
    "\n",
    "# For every region created from the model\n",
    "for region in regions:\n",
    "    \n",
    "    # Get the shape of each region\n",
    "    polygon = vertices[region]\n",
    "    \n",
    "    # Insert them into a list\n",
    "    shape = list(polygon.shape)\n",
    "    shape[0] += 1\n",
    "    \n",
    "    # Create and map the polygon \n",
    "    p = Polygon(np.append(polygon, polygon[0]).reshape(*shape)).intersection(mask)\n",
    "    poly = np.array(list(zip(p.boundary.coords.xy[0][:-1], p.boundary.coords.xy[1][:-1])))\n",
    "    \n",
    "    # Add and fill in the polygon onto the plot\n",
    "    new_vertices.append(poly)\n",
    "    plt.fill(*zip(*poly), alpha=0.4)\n",
    "    \n",
    "# Plot the points of the vectorization\n",
    "plt.plot(Vector2DResults[:,0], Vector2DResults[:,1], 'ko')\n",
    "\n",
    "# Annotate the points\n",
    "for i, txt in enumerate(VectorWords):\n",
    "    plt.annotate(txt, (Vector2DResults[i][0],Vector2DResults[i][1]), fontsize = 10)\n",
    "\n",
    "# Set the title of the diagram\n",
    "plt.title(\"Voronois Diagram of \" + searchTerm)\n",
    "\n",
    "# Display the diagram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to take notice that in the diagram there are points that are not words. Namely there are terms such as \"“\" or \"--\". The main reason why I did not remove them earlier is to demostrate that the cleaning (removing puncations and stop words) done eariler did not catch all cases. Therefore it is important to clean, manipulate, and customize the corpus to suit your needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we have look at reading in a text file and cleaning it. Finding all the sentences that contained a particular term. Using word embedding and PCA converted the relationship the search term had with every other word in sentences that it appeared in to plot it in a Voronoi Diagram."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
